{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cebe90ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected: 122 columns (including spectra pH and EC if present).\n",
      "Features going to be predicted: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# 1. Load Data\n",
    "# Replace 'your_dataset.csv' with your actual file path\n",
    "df = pd.read_csv('spectral_feature_data_notaveraged.csv')\n",
    "\n",
    "# 2. Define Targets (The columns you want to predict)\n",
    "# Update this list to match the exact column names in your CSV\n",
    "target_cols = [col for col in df.columns if col.startswith(\"p\")]\n",
    "\n",
    "# Select features: All columns that are NOT in the exclusion list\n",
    "# This assumes your CSV contains: Spectral_Cols, ph, ec, Target_Cols, and ID\n",
    "non_feature_cols = [col for col in df.columns if col.startswith('p4')]\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in non_feature_cols]\n",
    "\n",
    "#print(f\"{target_cols}\\n\\n{non_feature_cols}\\n\\n{feature_cols}\")\n",
    " \n",
    "#input features\n",
    "prediction_columns = [col for col in feature_cols if not col.startswith(\"p\")]\n",
    "prediction_columns.extend([\"p1.pH.index\", 'p1.EC.ds_m'])\n",
    "\n",
    "# Verify that 'ph' and 'ec' are actually in feature_cols\n",
    "# If your spectral columns are named 'Band_1', etc., and you have 'ph' and 'ec', \n",
    "# this logic automatically grabs them as long as they aren't in 'non_feature_cols'.\n",
    "print(f\"Features selected: {len(feature_cols)} columns (including spectra pH and EC if present).\")\n",
    "print(f\"Features going to be predicted: {len(feature_cols) - len(prediction_columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6db7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['400', '410', '420', '430', '440', '450', '460', '470', '480', '490',\n",
      "       ...\n",
      "       '1420', '1430', '1440', '1450', '1460', '1470', '1480', '1490',\n",
      "       'p1.pH.index', 'p1.EC.ds_m'],\n",
      "      dtype='object', length=112)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "X = df[prediction_columns]\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26af70bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process started at: Tue Dec 30 20:18:08 2025\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Process started at: {time.ctime(start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885635e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search with XGBoost...\n",
      "\n",
      "--- Processing Target: p1.pH.index ---\n",
      "Data points available for p1.pH.index: 44590\n",
      "Fitting 2 folds for each of 324 candidates, totalling 648 fits\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Grid Search with XGBoost...\")\n",
    "df_save = pd.DataFrame(['feature', 'mae', 'rmse', 'r2_score'] + list(param_grid.keys())).T\n",
    "\n",
    "# 5. Processing Loop for each Target\n",
    "for target in target_cols:\n",
    "    print(f\"\\n--- Processing Target: {target} ---\")\n",
    "    \n",
    "    # Check if target exists in dataframe\n",
    "    if target not in df.columns:\n",
    "        print(f\"Warning: Column '{target}' not found in CSV. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # --- FIX STARTS HERE ---\n",
    "    # 1. Create a mask for rows where the current target is NOT NaN\n",
    "    mask = df[target].notna()\n",
    "    \n",
    "    # 2. Apply this mask to both X and y so they remain aligned\n",
    "    #    We use .loc to grab the specific rows defined by the mask\n",
    "    y_clean = df.loc[mask, target]\n",
    "    X_clean = X.loc[mask]\n",
    "    \n",
    "    # Optional: Safety check for empty data\n",
    "    print(f\"Data points available for {target}: {len(y_clean)}\")\n",
    "    if len(y_clean) < 5: \n",
    "        print(f\"Skipping {target}: Not enough data points (found {len(y_clean)}).\")\n",
    "        continue\n",
    "    # --- FIX ENDS HERE ---\n",
    "    \n",
    "    # Split data (using the CLEAN variables)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize XGBoost Regressor\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    \n",
    "    # Run Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=2,      # 2-fold cross-validation\n",
    "        n_jobs=10, # Use all available processors\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        grid_search.fit(X_train, y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {target}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Get best results\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "    print(f\"Best Params for {target}: {best_params}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "    # Save Best Params to the DataFrame (Optional)\n",
    "    # Note: This adds the params to all rows, or you can save to a separate results list/dict\n",
    "    row = pd.DataFrame([[target, mse, rmse, r2] + [best_params[key] for key in param_grid.keys()]], columns=df_save.columns)\n",
    "    df_save = pd.concat([df_save, row], ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1339aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processes ended at: Tue Dec 30 19:01:10 2025\n",
      "Total processing time: 2296.3321101665497 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processes ended at: {time.ctime(time.time())}\")\n",
    "\n",
    "print(f\"Total processing time: {(time.time() - start_time)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8dff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete. Results saved to 'soil_properties_xgboost_results(averaged_cols).csv'.\n"
     ]
    }
   ],
   "source": [
    "# 7. Save the final results to CSV\n",
    "\n",
    "output_filename = 'soil_properties_xgboost_results(using more wavelengths).csv'\n",
    "df_save.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nProcessing complete. Results saved to '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf74ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
