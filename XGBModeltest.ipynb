{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cebe90ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected: 30 columns (including spectra pH and EC if present).\n",
      "Features going to be predicted: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. Load Data\n",
    "# Replace 'your_dataset.csv' with your actual file path\n",
    "df = pd.read_csv('spectral_feature_data_notaveraged.csv')\n",
    "\n",
    "# 2. Define Targets (The columns you want to predict)\n",
    "# Update this list to match the exact column names in your CSV\n",
    "target_cols = [col for col in df.columns if col.startswith(\"p\")]\n",
    "\n",
    "# Select features: All columns that are NOT in the exclusion list\n",
    "# This assumes your CSV contains: Spectral_Cols, ph, ec, Target_Cols, and ID\n",
    "non_feature_cols = [col for col in df.columns if col.startswith('p4')]\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in non_feature_cols]\n",
    "\n",
    "#print(f\"{target_cols}\\n\\n{non_feature_cols}\\n\\n{feature_cols}\")\n",
    "\n",
    "prediction_columns = [col for col in feature_cols if not col.startswith(\"p\")]\n",
    "prediction_columns.extend([\"p1.pH.index\", 'p1.EC.ds_m'])\n",
    "\n",
    "# Verify that 'ph' and 'ec' are actually in feature_cols\n",
    "# If your spectral columns are named 'Band_1', etc., and you have 'ph' and 'ec', \n",
    "# this logic automatically grabs them as long as they aren't in 'non_feature_cols'.\n",
    "print(f\"Features selected: {len(feature_cols)} columns (including spectra pH and EC if present).\")\n",
    "print(f\"Features going to be predicted: {len(feature_cols) - len(prediction_columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da6db7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['410', '435', '460', '485', '510', '535', '560', '585', '610', '645',\n",
      "       '680', '705', '730', '760', '810', '860', '900', '940', 'p1.pH.index',\n",
      "       'p1.EC.ds_m'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'subsample': [0.4, 0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.4, 0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "X = df[prediction_columns]\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26af70bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process started at: Tue Dec 30 12:54:03 2025\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Process started at: {time.ctime(start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885635e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search with XGBoost...\n",
      "\n",
      "--- Processing Target: p1.pH.index ---\n",
      "Data points available for p1.pH.index: 44590\n",
      "Fitting 3 folds for each of 2400 candidates, totalling 7200 fits\n",
      "Best Params for p1.pH.index: {'colsample_bytree': 1.0, 'learning_rate': 0.03, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.4}\n",
      "MSE: 0.0006\n",
      "RMSE: 0.0235\n",
      "R2 Score: 0.9997\n",
      "\n",
      "--- Processing Target: p1.EC.ds_m ---\n",
      "Data points available for p1.EC.ds_m: 21833\n",
      "Fitting 3 folds for each of 2400 candidates, totalling 7200 fits\n",
      "Best Params for p1.EC.ds_m: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.4}\n",
      "MSE: 0.0034\n",
      "RMSE: 0.0585\n",
      "R2 Score: 0.9694\n",
      "\n",
      "--- Processing Target: p1.Clay.wt_pct ---\n",
      "Data points available for p1.Clay.wt_pct: 0\n",
      "Skipping p1.Clay.wt_pct: Not enough data points (found 0).\n",
      "\n",
      "--- Processing Target: p1.Sand.wt_pct ---\n",
      "Data points available for p1.Sand.wt_pct: 23239\n",
      "Fitting 3 folds for each of 2400 candidates, totalling 7200 fits\n",
      "Best Params for p1.Sand.wt_pct: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 500, 'subsample': 0.4}\n",
      "MSE: 430.8610\n",
      "RMSE: 20.7572\n",
      "R2 Score: 0.3654\n",
      "\n",
      "--- Processing Target: p1.Silt.wt_pct ---\n",
      "Data points available for p1.Silt.wt_pct: 166\n",
      "Fitting 3 folds for each of 2400 candidates, totalling 7200 fits\n",
      "Best Params for p1.Silt.wt_pct: {'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 8, 'n_estimators': 100, 'subsample': 0.4}\n",
      "MSE: 261.2135\n",
      "RMSE: 16.1621\n",
      "R2 Score: 0.1866\n",
      "\n",
      "--- Processing Target: p2.N.wt_pct ---\n",
      "Data points available for p2.N.wt_pct: 40764\n",
      "Fitting 3 folds for each of 2400 candidates, totalling 7200 fits\n",
      "Best Params for p2.N.wt_pct: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 400, 'subsample': 0.4}\n",
      "MSE: 0.0309\n",
      "RMSE: 0.1758\n",
      "R2 Score: 0.7644\n",
      "\n",
      "--- Processing Target: p2.Zn.mg_kg ---\n",
      "Data points available for p2.Zn.mg_kg: 0\n",
      "Skipping p2.Zn.mg_kg: Not enough data points (found 0).\n",
      "\n",
      "--- Processing Target: p2.OC.wt_pct ---\n",
      "Data points available for p2.OC.wt_pct: 40764\n",
      "Fitting 3 folds for each of 2400 candidates, totalling 7200 fits\n",
      "Best Params for p2.OC.wt_pct: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 500, 'subsample': 0.4}\n",
      "MSE: 11.2277\n",
      "RMSE: 3.3508\n",
      "R2 Score: 0.8322\n",
      "\n",
      "--- Processing Target: p3.Fe.mg_kg ---\n",
      "Data points available for p3.Fe.mg_kg: 0\n",
      "Skipping p3.Fe.mg_kg: Not enough data points (found 0).\n",
      "\n",
      "--- Processing Target: p3.K.mg_kg ---\n",
      "Data points available for p3.K.mg_kg: 0\n",
      "Skipping p3.K.mg_kg: Not enough data points (found 0).\n",
      "\n",
      "--- Processing Target: p3.P.mg_kg ---\n",
      "Data points available for p3.P.mg_kg: 40764\n",
      "Fitting 3 folds for each of 2400 candidates, totalling 7200 fits\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Grid Search with XGBoost...\")\n",
    "df_save = pd.DataFrame(['feature', 'mae', 'rmse', 'r2_score'] + list(param_grid.keys())).T\n",
    "\n",
    "# 5. Processing Loop for each Target\n",
    "for target in target_cols:\n",
    "    print(f\"\\n--- Processing Target: {target} ---\")\n",
    "    \n",
    "    # Check if target exists in dataframe\n",
    "    if target not in df.columns:\n",
    "        print(f\"Warning: Column '{target}' not found in CSV. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # --- FIX STARTS HERE ---\n",
    "    # 1. Create a mask for rows where the current target is NOT NaN\n",
    "    mask = df[target].notna()\n",
    "    \n",
    "    # 2. Apply this mask to both X and y so they remain aligned\n",
    "    #    We use .loc to grab the specific rows defined by the mask\n",
    "    y_clean = df.loc[mask, target]\n",
    "    X_clean = X.loc[mask]\n",
    "    \n",
    "    # Optional: Safety check for empty data\n",
    "    print(f\"Data points available for {target}: {len(y_clean)}\")\n",
    "    if len(y_clean) < 5: \n",
    "        print(f\"Skipping {target}: Not enough data points (found {len(y_clean)}).\")\n",
    "        continue\n",
    "    # --- FIX ENDS HERE ---\n",
    "    \n",
    "    # Split data (using the CLEAN variables)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize XGBoost Regressor\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    \n",
    "    # Run Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=3,      # 3-fold cross-validation\n",
    "        n_jobs=-1, # Use all available processors\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        grid_search.fit(X_train, y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {target}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Get best results\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Best Params for {target}: {best_params}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "    # Save Best Params to the DataFrame (Optional)\n",
    "    # Note: This adds the params to all rows, or you can save to a separate results list/dict\n",
    "    row = pd.DataFrame([[target, mse, rmse, r2] + [best_params[key] for key in param_grid.keys()]], columns=df_save.columns)\n",
    "    df_save = pd.concat([df_save, row], ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1339aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Processes ended at: {time.ctime(time.time())}\")\n",
    "\n",
    "print(f\"Total processing time: {(time.time() - start_time)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8dff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete. Results saved to 'soil_properties_xgboost_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "# 7. Save the final results to CSV\n",
    "\n",
    "output_filename = 'soil_properties_xgboost_results.csv'\n",
    "df_save.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nProcessing complete. Results saved to '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf74ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
