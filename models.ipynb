{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a110b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\jagda\\anaconda3\\lib\\site-packages (2.20.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jagda\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\jagda\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jagda\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: xgboost in c:\\users\\jagda\\anaconda3\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: pillow in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Docstring for models.ipynb\n",
    "using spectral_feature_data.csv to train ML models and check their performance\n",
    "the training and testing split is 80% - 20%\n",
    "training is done using cross validation with 5 folds\n",
    "models going to be used:\n",
    "- Random Forest regressor\n",
    "- XGBoost regressor\n",
    "- CNN\n",
    "\n",
    "'''\n",
    "%pip install tensorflow scikit-learn pandas numpy tensorflow xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras import models, layers, Input\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import xgboost as xgb\n",
    "\n",
    "raw_features = pd.read_csv('spectral_feature_data.csv')\n",
    "raw_features.drop([\"p3.Fe.mg_kg\", \"p2.Zn.mg_kg\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca7e8aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44736, 34)\n",
      "Total rows in most occurance columns (>40k):\n",
      "p1.pH.index \t\t 44590\n",
      "p2.N.wt_pct \t\t 40931\n",
      "p2.OC.wt_pct \t\t 44624\n",
      "p3.K.mg_kg \t\t 44488\n",
      "p3.P.mg_kg \t\t 40764\n",
      "\n",
      "\n",
      "Total Rows in medium occurance columns (20k-40k):\n",
      "p1.EC.ds_m         21833\n",
      "p1.Sand.wt_pct     27139\n",
      "p4.CEC.cmolc_kg    22685\n",
      "p4.CF.wt_pct       23237\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Total Rows in hard occurance columns (<20k):\n",
      "p1.Clay.wt_pct          3915\n",
      "p1.Silt.wt_pct          3901\n",
      "p3.S.wt_pct              167\n",
      "p4.BD.g_cm3             1162\n",
      "p4.WR_10kPa.wt_pct       924\n",
      "p4.WR_1500kPa.wt_pct     967\n",
      "p4.WR_33kPa.wt_pct       919\n",
      "dtype: int64\n",
      "\n",
      "Spectral Wavelengths: (No. of readings: 18)\n",
      "\n",
      "['410', '435', '460', '485', '510', '535', '560', '585', '610', '645', '680', '705', '730', '760', '810', '860', '900', '940']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DEFINING EASY, MEDIUM, HARD FEATURES BASED ON OCCURANCE\n",
    "total_rows = raw_features.shape[0]\n",
    "df = pd.DataFrame()\n",
    "print(raw_features.shape)\n",
    "df_p_cols = [col for col in raw_features.columns if col.startswith('p')]\n",
    "easy_features = []\n",
    "print(\"Total rows in most occurance columns (>40k):\")\n",
    "for col in df_p_cols:\n",
    "    if raw_features[col].isna().sum() < 10000:\n",
    "        print(col,\"\\t\\t\", total_rows - raw_features[col].isna().sum())\n",
    "        easy_features.append(col)\n",
    "\n",
    "print(\"\\n\")\n",
    "medium_features = [col for col in df_p_cols if col not in easy_features and raw_features[col].notna().sum() > 20000]\n",
    "print(\"Total Rows in medium occurance columns (20k-40k):\")\n",
    "print(raw_features[medium_features].notna().sum())\n",
    "hard_features = [col for col in df_p_cols if col not in easy_features and col not in medium_features]\n",
    "print(\"\\n\")\n",
    "print(\"Total Rows in hard occurance columns (<20k):\") \n",
    "print(raw_features[hard_features].notna().sum())\n",
    "spectral_cols = [col for col in raw_features.columns if col[0] != 'p']\n",
    "print(f\"\\nSpectral Wavelengths: (No. of readings: {len(spectral_cols)})\\n\\n{spectral_cols}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b5e519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p1.EC.ds_m', 'p1.Sand.wt_pct', 'p4.CEC.cmolc_kg', 'p4.CF.wt_pct']\n"
     ]
    }
   ],
   "source": [
    "print(medium_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa8c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROW-WISE STANDARDIZATION (Standard Normal Variate - SNV) FOR SPECTRA\n",
    "# ======================================================\n",
    "def apply_snv(input_data):\n",
    "    \"\"\"\n",
    "    Standard Normal Variate (SNV):\n",
    "    Subtracts the Row Mean and divides by Row Std Dev.\n",
    "    \"\"\"\n",
    "    # axis=1 means \"calculate across the row\"\n",
    "    row_mean = input_data.mean(axis=1)\n",
    "    row_std = input_data.std(axis=1)\n",
    "    \n",
    "    # We use numpy broadcasting to subtract/divide\n",
    "    # (data - mean) / std\n",
    "    snv_data = (input_data.sub(row_mean, axis=0)).div(row_std, axis=0)\n",
    "    return snv_data\n",
    "\n",
    "# Apply only to spectral columns\n",
    "df_spectra_snv = apply_snv(raw_features[spectral_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e673902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to synthesize data by adding noise to existing data points\n",
    "#helps to increase dataset size for better training of models especially deep learning models like CNNs\n",
    "def augment_all_data(X_s, y, X_ph=None, X_ec=None, min_samples=500):\n",
    "    \"\"\"\n",
    "    Augments Spectral data + pH + EC with sensor-specific noise profiles.\n",
    "    \"\"\"\n",
    "    n_samples = X_s.shape[0]\n",
    "    \n",
    "    # 1. Skip if we have enough data\n",
    "    if n_samples >= min_samples:\n",
    "        return X_s, y, X_ph, X_ec\n",
    "        \n",
    "    # --- NOISE GENERATION ---\n",
    "    \n",
    "    # A. Spectral Noise (Background electronic noise)\n",
    "    # 1% of the standard deviation of the spectra\n",
    "    spec_noise_level = 0.01 * np.std(X_s) \n",
    "    noise_s = np.random.normal(0, spec_noise_level, X_s.shape)\n",
    "    \n",
    "    # B. pH Noise (Absolute Error)\n",
    "    # Standard pH probe accuracy is often +/- 0.02 to 0.05\n",
    "    if X_ph is not None:\n",
    "        noise_ph = np.random.normal(0, 0.05, X_ph.shape) # 0.05 pH unit variance\n",
    "    \n",
    "    # C. EC Noise (Relative Error)\n",
    "    # EC sensors usually have % error (e.g. 2%)\n",
    "    if X_ec is not None:\n",
    "        # Noise is 2% of the actual reading value\n",
    "        noise_ec = X_ec * np.random.normal(0, 0.02, X_ec.shape)\n",
    "\n",
    "    # --- APPLY & CONCATENATE ---\n",
    "    \n",
    "    # Create Noisy Copies\n",
    "    X_s_new  = X_s + noise_s\n",
    "    X_ph_new = (X_ph + noise_ph) if X_ph is not None else None\n",
    "    X_ec_new = (X_ec + noise_ec) if X_ec is not None else None\n",
    "    \n",
    "    # Concatenate (Original + Noisy)\n",
    "    X_s_aug = np.concatenate([X_s, X_s_new], axis=0)\n",
    "    y_aug   = np.concatenate([y, y], axis=0)\n",
    "    \n",
    "    X_ph_aug = np.concatenate([X_ph, X_ph_new], axis=0) if X_ph is not None else None\n",
    "    X_ec_aug = np.concatenate([X_ec, X_ec_new], axis=0) if X_ec is not None else None\n",
    "    \n",
    "    return X_s_aug, y_aug, X_ph_aug, X_ec_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f04d62e",
   "metadata": {},
   "source": [
    "For training the data, we will use a cnn on the easy features directly getting a 1D vector as output.\\n\n",
    "For the medium and hard data we will train individual models per feature getting a 0D output. \\n\n",
    "We will try using xgboost, and CNNs to see which model gives the best. \\n\n",
    "Final output will be a vector concatenating all of the outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7e4467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_flexible_model(input_shape, use_ph=False, use_ec=False):\n",
    "    \"\"\"\n",
    "    Builds a model with optional pH and EC inputs.\n",
    "    \"\"\"\n",
    "    inputs_list = []\n",
    "    features_to_concat = []\n",
    "\n",
    "    # --- 1. Spectral Branch (Always Present) ---\n",
    "    input_spec = Input(shape=input_shape, name='spectral_in')\n",
    "    inputs_list.append(input_spec)\n",
    "    \n",
    "    # Feature extraction logic\n",
    "    x = layers.Conv1D(5, 3, activation='relu')(input_spec)\n",
    "    #x = layers.Conv1D(5, 3, activation='relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    '''\n",
    "    2nd cnn:\n",
    "    x = layers.Conv1D(16, 3, activation='relu')(input_spec)\n",
    "    x = layers.Conv1D(8, 3, activation='relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)'''\n",
    "    features_to_concat.append(x)\n",
    "\n",
    "    # --- 2. Conditional pH Branch ---\n",
    "    if use_ph:\n",
    "        input_ph = Input(shape=(1,), name='ph_in')\n",
    "        inputs_list.append(input_ph)\n",
    "        features_to_concat.append(input_ph)\n",
    "\n",
    "    # --- 3. Conditional EC Branch ---\n",
    "    if use_ec:\n",
    "        input_ec = Input(shape=(1,), name='ec_in')\n",
    "        inputs_list.append(input_ec)\n",
    "        features_to_concat.append(input_ec)\n",
    "\n",
    "    # --- 4. Merge & Output ---\n",
    "    # If we have more than one feature source, we concatenate\n",
    "    if len(features_to_concat) > 1:\n",
    "        x = layers.Concatenate()(features_to_concat)\n",
    "    else:\n",
    "        x = features_to_concat[0] # Just the spectral features\n",
    "\n",
    "    # Regression Head\n",
    "    ''' 2nd cnn\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(8, activation='relu')(x)\n",
    "    output = layers.Dense(1, name='output')(x)'''\n",
    "    \n",
    "    #x = layers.Dense(5, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    #x = layers.Dense(8, activation='relu')(x)\n",
    "    output = layers.Dense(1, name='output')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs_list, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42350582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44736, 34)\n",
      "['p2.N.wt_pct', 'p2.OC.wt_pct', 'p3.K.mg_kg', 'p3.P.mg_kg', 'p1.EC.ds_m', 'p1.Sand.wt_pct', 'p1.Clay.wt_pct', 'p1.Silt.wt_pct', 'p3.S.wt_pct']\n",
      "{'p2.N.wt_pct': array([ nan,  nan,  nan, ..., 0.09, 0.06, 0.22]), 'p2.OC.wt_pct': array([0.95, 0.42, 0.38, ..., 0.67, 0.57, 1.81]), 'p3.K.mg_kg': array([  78.26      ,    0.        ,    0.        , ...,  958.98424472,\n",
      "       1022.83646222, 1117.31372449]), 'p3.P.mg_kg': array([ nan,  nan,  nan, ..., 29.5,  0. ,  0. ]), 'p1.EC.ds_m': array([   nan,    nan,    nan, ..., 0.1509, 0.155 , 0.1517]), 'p1.Sand.wt_pct': array([69. , 64.6, 61.3, ..., 33. , 45. , 30. ]), 'p1.Clay.wt_pct': array([15. , 20.9, 25.9, ...,  nan,  nan,  nan]), 'p1.Silt.wt_pct': array([15.9, 14.5, 12.9, ...,  nan,  nan,  nan]), 'p3.S.wt_pct': array([nan, nan, nan, ..., nan, nan, nan])}\n",
      "[[        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " ...\n",
      " [-0.33071381]\n",
      " [-0.31843349]\n",
      " [-0.32831765]] [[-1.04020046]\n",
      " [-0.89525692]\n",
      " [-0.75031338]\n",
      " ...\n",
      " [ 1.17018856]\n",
      " [ 1.29701416]\n",
      " [ 1.34412081]]\n"
     ]
    }
   ],
   "source": [
    "#Defining dataset, Scaling the values\n",
    "# dataset contains: \n",
    "# 1. scaled spectral columns\n",
    "# 2. ph and ec columns\n",
    "# 3. other target features\n",
    "\n",
    "features = easy_features + medium_features + hard_features\n",
    "df_final = pd.concat([df_spectra_snv, raw_features[features]], axis=1) #has snv spectral with non scaled other features\n",
    "print(df_final.shape)\n",
    "\n",
    "#these are the scaled versions of ph and ec to be used as inputs when required\n",
    "scaler = StandardScaler()\n",
    "ec_scaled = scaler.fit_transform(raw_features[[\"p1.EC.ds_m\"]])\n",
    "ph_scaled = scaler.fit_transform(raw_features[[\"p1.pH.index\"]])\n",
    "\n",
    "feature_vals = {}\n",
    "features.remove(\"p1.pH.index\")\n",
    "features = [f for f in features if not f.startswith(\"p4\")]\n",
    "print(features)\n",
    "\n",
    "for feature in features:\n",
    "    feature_vals[feature] = df_final[feature].values\n",
    "print(feature_vals)\n",
    "print(ec_scaled, ph_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee2f4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_multi_feature_experiment(data_dictionary, X_spectral, X_ph, X_ec):\n",
    "    \"\"\"\n",
    "    data_dictionary: {'Nitrogen': y_nitrogen_array, 'Phosphorus': y_phosphorus_array, ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Setup Storage\n",
    "    master_results = []\n",
    "    \n",
    "    # 2. Define Configs\n",
    "    configs = [\n",
    "        {\"name\": \"Spectral + pH\",      \"ph\": True,  \"ec\": False},\n",
    "        {\"name\": \"Spectral + EC\",      \"ph\": False, \"ec\": True},\n",
    "        {\"name\": \"Spectral + pH + EC\", \"ph\": True,  \"ec\": True}\n",
    "    ]\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    X_ph_temp = X_ph.copy() # Make a copy so we don't change original data\n",
    "    X_ph_temp[X_ph_temp == 0] = np.nan\n",
    "\n",
    "    # If X_ec has 0s that act as NaNs:\n",
    "    X_ec_temp = X_ec.copy()\n",
    "    X_ec_temp[X_ec_temp == 0] = np.nan\n",
    "\n",
    "    \n",
    "    # 3. Outer Loop: Iterate through Features (Nitrogen, Potassium, etc.)\n",
    "    # tqdm makes a nice progress bar\n",
    "    for feature_name, y_full in tqdm(data_dictionary.items(), desc=\"Processing Features\"):\n",
    "        \n",
    "        print(f\"\\n--- Starting Feature: {feature_name} ---\")\n",
    "        # --- 2. Create Intelligent Mask for NaN Removal ---\n",
    "        # We keep a row ONLY if: Target is valid AND pH is valid AND EC is valid\n",
    "        # This ensures all 4 configs train on the exact same dataset for fair comparison.\n",
    "        \n",
    "        # Check where data exists (True if valid, False if NaN)\n",
    "        # Assuming inputs are 1D arrays or flattened. \n",
    "        # If X_ph/X_ec are (N,1) shape, we use .flatten() just for the check\n",
    "        \n",
    "        y_full[y_full == 0] = np.nan \n",
    "        \n",
    "        # If X_ph has 0s that act as NaNs:\n",
    "\n",
    "        \n",
    "        # --- 2. Create Intelligent Mask (Same as before) ---\n",
    "        valid_y  = ~np.isnan(y_full).flatten()\n",
    "        valid_ph = ~np.isnan(X_ph_temp).flatten() # Use the temp version\n",
    "        valid_ec = ~np.isnan(X_ec_temp).flatten() # Use the temp version\n",
    "        \n",
    "\n",
    "        printed = False  # To ensure we print dropped rows info only once per feature\n",
    "        # 4. Middle Loop: Configurations\n",
    "        for config in configs:\n",
    "            \n",
    "            fold_maes = []\n",
    "            fold_r2s = []\n",
    "            fold_mapes = []\n",
    "\n",
    "            mask = valid_y.copy() # Good practice to copy, though not strictly required\n",
    "            \n",
    "            # 2. Add pH requirement ONLY if the config needs it\n",
    "            if config['ph']:\n",
    "                mask = mask & valid_ph\n",
    "                \n",
    "            # 3. Add EC requirement ONLY if the config needs it\n",
    "            if config['ec']:\n",
    "                mask = mask & valid_ec\n",
    "        \n",
    "            # Apply Mask to Create \"Clean\" Subsets for this Feature\n",
    "            X_s_clean = X_spectral[mask]\n",
    "            X_p_clean = X_ph_temp[mask]\n",
    "            X_e_clean = X_ec_temp[mask]\n",
    "            y_clean   = y_full[mask]\n",
    "\n",
    "            n_removed = len(y_full) - len(y_clean)\n",
    "            if n_removed > 0:\n",
    "                    # Optional: Print info if data was dropped\n",
    "                tqdm.write(f\"  > Feature '{feature_name}': Dropped {n_removed} rows due to NaNs.\")\n",
    "\n",
    "                # If data is empty after cleaning, skip\n",
    "            if len(y_clean) < 100: \n",
    "                print(f\"  ! Skipping {feature_name}: Not enough data points after cleaning.\")\n",
    "                break\n",
    "                \n",
    "            # 5. Inner Loop: 5-Fold Cross Validation\n",
    "            if len(X_s_clean) < 10:\n",
    "                continue\n",
    "\n",
    "            for train_idx, val_idx in kf.split(X_s_clean):\n",
    "                \n",
    "                # --- STEP 1: EXTRACT RAW DATA ---\n",
    "                x_train_s, x_val_s = X_s_clean[train_idx], X_s_clean[val_idx]\n",
    "                y_train, y_val     = y_clean[train_idx],   y_clean[val_idx]\n",
    "                \n",
    "                # Extract pH (or set to None if not used)\n",
    "                if config['ph']:\n",
    "                    x_train_p = X_p_clean[train_idx]\n",
    "                    x_val_p   = X_p_clean[val_idx]\n",
    "                else:\n",
    "                    x_train_p, x_val_p = None, None\n",
    "\n",
    "                # Extract EC (or set to None if not used)\n",
    "                if config['ec']:\n",
    "                    x_train_e = X_e_clean[train_idx]\n",
    "                    x_val_e   = X_e_clean[val_idx]\n",
    "                else:\n",
    "                    x_train_e, x_val_e = None, None\n",
    "\n",
    "                # --- STEP 2: AUGMENTATION (Modify the variables) ---\n",
    "                # This updates x_train_s, y_train, etc. to include the noisy copies\n",
    "                x_train_s, y_train, x_train_p, x_train_e = augment_all_data(\n",
    "                    x_train_s, y_train, x_train_p, x_train_e, min_samples=500\n",
    "                )\n",
    "\n",
    "                # --- STEP 3: BUILD INPUT LISTS (Using the NEW augmented variables) ---\n",
    "                \n",
    "                # Training Inputs (Augmented)\n",
    "                train_inputs = [x_train_s]\n",
    "                if config['ph']:\n",
    "                    train_inputs.append(x_train_p)\n",
    "                if config['ec']:\n",
    "                    train_inputs.append(x_train_e)\n",
    "                    \n",
    "                # Validation Inputs (Original - never augmented)\n",
    "                val_inputs = [x_val_s]\n",
    "                if config['ph']:\n",
    "                    val_inputs.append(x_val_p)\n",
    "                if config['ec']:\n",
    "                    val_inputs.append(x_val_e)\n",
    "\n",
    "\n",
    "                # B. Build & Train\n",
    "                # (Make sure build_flexible_model is defined in your scope)\n",
    "                model = build_flexible_model(\n",
    "                    input_shape=(X_s_clean.shape[1], 1),\n",
    "                    use_ph=config['ph'],\n",
    "                    use_ec=config['ec']\n",
    "                )\n",
    "                \n",
    "                model.fit(\n",
    "                    x=train_inputs, y=y_train,\n",
    "                    validation_data=(val_inputs, y_val),\n",
    "                    epochs=30, batch_size=32, verbose=0\n",
    "                )\n",
    "                \n",
    "                # C. Evaluate\n",
    "                preds = model.predict(val_inputs, verbose=0).flatten()\n",
    "\n",
    "                mape_val = np.mean(np.abs((y_val - preds) / (y_val + 1e-6))) * 100\n",
    "                \n",
    "                fold_maes.append(mean_absolute_error(y_val, preds))\n",
    "                fold_r2s.append(r2_score(y_val, preds))\n",
    "                fold_mapes.append(mape_val)\n",
    "            \n",
    "            # 6. Aggregate Results for this Config\n",
    "            avg_mae = np.mean(fold_maes)\n",
    "            avg_r2 = np.mean(fold_r2s)\n",
    "            \n",
    "            \n",
    "            # Add to list\n",
    "            master_results.append({\n",
    "                \"Feature\": feature_name,\n",
    "                \"Config\": config['name'],\n",
    "                \"MAE\": avg_mae,\n",
    "                \"R2\": avg_r2,\n",
    "                \"Samples_Used\": len(y_clean),\n",
    "                \"Error_%\": np.mean(fold_mapes)\n",
    "            })\n",
    "            \n",
    "         # 7. SAVE CHECKPOINT (Crucial for long runs)\n",
    "         # Saves to CSV after every feature is done, so you never lose work.\n",
    "        df_temp = pd.DataFrame(master_results)\n",
    "        df_temp.to_csv(\"experiment_results_checkpoint(reduced neurons)(1).csv\", index=False)\n",
    "        print(f\"Saved checkpoint for {feature_name}\")\n",
    "\n",
    "    return pd.DataFrame(master_results)\n",
    "\n",
    "# --- How to Run It ---\n",
    "\n",
    "# 1. Create a dictionary of your targets\n",
    "# y_targets = {\n",
    "#     \"Nitrogen\": y_n,\n",
    "#     \"Phosphorus\": y_p,\n",
    "#     \"Potassium\": y_k,\n",
    "#     # ... add all 16 features here\n",
    "# }\n",
    "\n",
    "# 2. Run\n",
    "# final_df = run_multi_feature_experiment(y_targets, X_s_clean, X_ph, X_ec)\n",
    "# print(final_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3117f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data_for_config(X_s, y_full, X_ph, X_ec, config):\n",
    "    \"\"\"\n",
    "    1. filters rows where Target is NaN.\n",
    "    2. checks config to see if pH/EC are needed, and filters for those too.\n",
    "    Returns the cleaned subsets ready for splitting.\n",
    "    \"\"\"\n",
    "    # Base Mask: Target must exist\n",
    "    mask = ~np.isnan(y_full).flatten()\n",
    "    \n",
    "    # Optional Masks: Only apply if the config requires that sensor\n",
    "    if config['ph']:\n",
    "        mask = mask & ~np.isnan(X_ph).flatten()\n",
    "    if config['ec']:\n",
    "        mask = mask & ~np.isnan(X_ec).flatten()\n",
    "        \n",
    "    return X_s[mask], y_full[mask], X_ph[mask], X_ec[mask]\n",
    "\n",
    "\n",
    "def format_inputs(x_s, x_p, x_e, config, model_type):\n",
    "    \"\"\"\n",
    "    Prepares the final input shape based on the model type.\n",
    "    - XGBoost: Stacks everything into one big 2D matrix (np.hstack).\n",
    "    - CNN: Returns a list of separate arrays [Spectral, pH, EC].\n",
    "    \"\"\"\n",
    "    if model_type == 'xgboost':\n",
    "        # Start with Spectral\n",
    "        X_final = x_s\n",
    "        # Glue columns to the right side\n",
    "        if config['ph']:\n",
    "            X_final = np.hstack([X_final, x_p.reshape(-1, 1)])\n",
    "        if config['ec']:\n",
    "            X_final = np.hstack([X_final, x_e.reshape(-1, 1)])\n",
    "        return X_final\n",
    "\n",
    "    elif model_type == 'cnn':\n",
    "        # Create a list of inputs\n",
    "        X_list = [x_s]\n",
    "        if config['ph']: X_list.append(x_p)\n",
    "        if config['ec']: X_list.append(x_e)\n",
    "        return X_list\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "def train_xgboost_fold(X_train, y_train, X_val, y_val, params):\n",
    "    \"\"\"Specific logic to train one XGBoost model.\"\"\"\n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=500,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        **params # Unpack dynamic params like depth, learning_rate\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    return model.predict(X_val)\n",
    "\n",
    "def train_cnn_fold(X_train, y_train, X_val, y_val, params):\n",
    "    \"\"\"Specific logic to train one CNN model.\"\"\"\n",
    "    # Note: Assumes build_flexible_model is available in scope\n",
    "    input_shape = (X_train[0].shape[1], 1) # (Wavelengths, 1)\n",
    "    \n",
    "    model = build_flexible_model(\n",
    "        input_shape=input_shape,\n",
    "        use_ph=params['use_ph'], \n",
    "        use_ec=params['use_ec']\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        x=X_train, y=y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=30, batch_size=32, verbose=0\n",
    "    )\n",
    "    return model.predict(X_val, verbose=0).flatten()\n",
    "\n",
    "def process_single_fold(X_s, y, X_p, X_e, train_idx, val_idx, config, model_type, params):\n",
    "    \"\"\"\n",
    "    Orchestrates the workflow for ONE fold: \n",
    "    Split -> Augment -> Format -> Train -> Predict.\n",
    "    \"\"\"\n",
    "    # 1. Split Raw Data\n",
    "    x_train_s, x_val_s = X_s[train_idx], X_s[val_idx]\n",
    "    y_train, y_val     = y[train_idx],   y[val_idx]\n",
    "    \n",
    "    # Handle Optional Inputs (Extract or set to None)\n",
    "    x_train_p = X_p[train_idx] if config['ph'] else None\n",
    "    x_val_p   = X_p[val_idx]   if config['ph'] else None\n",
    "    \n",
    "    x_train_e = X_e[train_idx] if config['ec'] else None\n",
    "    x_val_e   = X_e[val_idx]   if config['ec'] else None\n",
    "\n",
    "    # 2. Augment (Training Only)\n",
    "    # Assumes augment_all_data is defined\n",
    "    x_train_s, y_train, x_train_p, x_train_e = augment_all_data(\n",
    "        x_train_s, y_train, x_train_p, x_train_e, min_samples=500\n",
    "    )\n",
    "\n",
    "    # 3. Format Inputs (Stacking vs List)\n",
    "    train_input = format_inputs(x_train_s, x_train_p, x_train_e, config, model_type)\n",
    "    val_input   = format_inputs(x_val_s, x_val_p, x_val_e, config, model_type)\n",
    "\n",
    "    # 4. Train & Predict\n",
    "    if model_type == 'xgboost':\n",
    "        preds = train_xgboost_fold(train_input, y_train, val_input, y_val, params)\n",
    "    elif model_type == 'cnn':\n",
    "        # Pass config flags to CNN params\n",
    "        params['use_ph'] = config['ph']\n",
    "        params['use_ec'] = config['ec']\n",
    "        preds = train_cnn_fold(train_input, y_train, val_input, y_val, params)\n",
    "        \n",
    "    # 5. Calculate Metrics\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    r2  = r2_score(y_val, preds)\n",
    "    mape = np.mean(np.abs((y_val - preds) / (y_val + 1e-6))) * 100\n",
    "    \n",
    "    return mae, r2, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28fef19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_modular_experiment(target_dict, X_spectral, X_ph, X_ec, model_type, save_file, **model_params):\n",
    "    \"\"\"\n",
    "    Main Driver Function.\n",
    "    model_type: 'xgboost' or 'cnn'\n",
    "    model_params: dictionary of parameters (e.g. depth=6, learning_rate=0.05)\n",
    "    \"\"\"\n",
    "    master_results = []\n",
    "    \n",
    "    configs = [\n",
    "        {\"name\": \"Spectral + pH + EC\", \"ph\": True,  \"ec\": True}\n",
    "    ]\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # --- Loop 1: Features ---\n",
    "    for feature_name, y_full in tqdm(target_dict.items(), desc=f\"Running {model_type}\"):\n",
    "        \n",
    "        # --- Loop 2: Configurations ---\n",
    "        for config in configs:\n",
    "            \n",
    "            # A. Prepare Data (Helper Function)\n",
    "            X_s_curr, y_curr, X_p_curr, X_e_curr = get_clean_data_for_config(\n",
    "                X_spectral, y_full, X_ph, X_ec, config\n",
    "            )\n",
    "            \n",
    "            if len(y_curr) < 10: continue\n",
    "\n",
    "            # B. Cross Validation (Inner Loop)\n",
    "            fold_maes, fold_r2s, fold_mapes = [], [], []\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X_s_curr):\n",
    "                # Call the worker helper\n",
    "                mae, r2, mape = process_single_fold(\n",
    "                    X_s_curr, y_curr, X_p_curr, X_e_curr, \n",
    "                    train_idx, val_idx, \n",
    "                    config, model_type, model_params\n",
    "                )\n",
    "                \n",
    "                fold_maes.append(mae)\n",
    "                fold_r2s.append(r2)\n",
    "                fold_mapes.append(mape)\n",
    "\n",
    "            # C. Aggregate & Save\n",
    "            master_results.append({\n",
    "                \"Feature\": feature_name,\n",
    "                \"Model\": model_type,\n",
    "                \"Config\": config['name'],\n",
    "                \"MAE\": np.mean(fold_maes),\n",
    "                \"R2\": np.mean(fold_r2s),\n",
    "                \"Error_%\": np.mean(fold_mapes),\n",
    "                \"Samples_Used\": len(y_curr)\n",
    "            })\n",
    "            \n",
    "            # Checkpoint Save\n",
    "            pd.DataFrame(master_results).to_csv(save_file, index=False)\n",
    "\n",
    "    return pd.DataFrame(master_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10de53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dynamic_xgboost_experiment(target_dict, X_spectral, X_ph, X_ec, depth, save_file):\n",
    "    master_results = []\n",
    "    \n",
    "    configs = [\n",
    "        {\"name\": \"Spectral Only\",      \"ph\": False, \"ec\": False},\n",
    "        {\"name\": \"Spectral + pH\",      \"ph\": True,  \"ec\": False},\n",
    "        {\"name\": \"Spectral + EC\",      \"ph\": False, \"ec\": True},\n",
    "        {\"name\": \"Spectral + pH + EC\", \"ph\": True,  \"ec\": True}\n",
    "    ]\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for feature_name, y_full in tqdm(target_dict.items(), desc=\"XGBoost Features\"):\n",
    "        \n",
    "        # Base Valid Mask (Target must exist)\n",
    "        base_valid_y = ~np.isnan(y_full).flatten()\n",
    "        \n",
    "        # Temp copies for masking checks\n",
    "        X_ph_temp = X_ph.copy()\n",
    "        X_ec_temp = X_ec.copy()\n",
    "        valid_ph = ~np.isnan(X_ph_temp).flatten()\n",
    "        valid_ec = ~np.isnan(X_ec_temp).flatten()\n",
    "        \n",
    "        for config in configs:\n",
    "            fold_maes = []\n",
    "            fold_r2s  = []\n",
    "            fold_mapes = []\n",
    "            \n",
    "            # 1. Build Dynamic Mask\n",
    "            mask = base_valid_y.copy()\n",
    "            if config['ph']: mask = mask & valid_ph\n",
    "            if config['ec']: mask = mask & valid_ec\n",
    "            \n",
    "            # 2. Extract CLEAN subsets\n",
    "            X_s_curr = X_spectral[mask]\n",
    "            y_curr   = y_full[mask]\n",
    "            X_p_curr = X_ph_temp[mask]\n",
    "            X_e_curr = X_ec_temp[mask]\n",
    "            \n",
    "            if len(y_curr) < 10: continue\n",
    "\n",
    "            # --- START CV LOOP ---\n",
    "            for train_idx, val_idx in kf.split(X_s_curr):\n",
    "                \n",
    "                # A. EXTRACT Raw Components\n",
    "                x_train_s, x_val_s = X_s_curr[train_idx], X_s_curr[val_idx]\n",
    "                y_train, y_val     = y_curr[train_idx],   y_curr[val_idx]\n",
    "                \n",
    "                # Handle optional inputs\n",
    "                if config['ph']:\n",
    "                    x_train_p, x_val_p = X_p_curr[train_idx], X_p_curr[val_idx]\n",
    "                else:\n",
    "                    x_train_p, x_val_p = None, None\n",
    "                    \n",
    "                if config['ec']:\n",
    "                    x_train_e, x_val_e = X_e_curr[train_idx], X_e_curr[val_idx]\n",
    "                else:\n",
    "                    x_train_e, x_val_e = None, None\n",
    "\n",
    "                # B. AUGMENTATION (Training Data Only)\n",
    "                x_train_s, y_train, x_train_p, x_train_e = augment_all_data(\n",
    "                    x_train_s, y_train, x_train_p, x_train_e, min_samples=500\n",
    "                )\n",
    "                \n",
    "                # C. STACKING (Build Matrices)\n",
    "                \n",
    "                # 1. Training Matrix (Augmented)\n",
    "                X_train_final = x_train_s\n",
    "                if config['ph']:\n",
    "                    X_train_final = np.hstack([X_train_final, x_train_p.reshape(-1, 1)])\n",
    "                if config['ec']:\n",
    "                    X_train_final = np.hstack([X_train_final, x_train_e.reshape(-1, 1)])\n",
    "                    \n",
    "                # 2. Validation Matrix (Original)\n",
    "                X_val_final = x_val_s\n",
    "                if config['ph']:\n",
    "                    X_val_final = np.hstack([X_val_final, x_val_p.reshape(-1, 1)])\n",
    "                if config['ec']:\n",
    "                    X_val_final = np.hstack([X_val_final, x_val_e.reshape(-1, 1)])\n",
    "\n",
    "                # D. TRAIN\n",
    "                model = xgb.XGBRegressor(\n",
    "                    objective='reg:squarederror',\n",
    "                    n_estimators=500, \n",
    "                    learning_rate=0.05, \n",
    "                    max_depth=depth,\n",
    "                    subsample=0.8, \n",
    "                    colsample_bytree=0.8, \n",
    "                    n_jobs=-1, \n",
    "                    random_state=42,\n",
    "                    early_stopping_rounds=20\n",
    "                )\n",
    "                \n",
    "                model.fit(\n",
    "                    X_train_final, y_train,\n",
    "                    eval_set=[(X_val_final, y_val)],\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                # E. EVALUATE (Corrected Order)\n",
    "                # 1. Predict FIRST\n",
    "                preds = model.predict(X_val_final)\n",
    "                \n",
    "                # 2. Calculate Metrics SECOND\n",
    "                mae = mean_absolute_error(y_val, preds)\n",
    "                r2  = r2_score(y_val, preds)\n",
    "                # Safe MAPE calculation\n",
    "                mape_val = np.mean(np.abs((y_val - preds) / (y_val + 1e-6))) * 100\n",
    "\n",
    "                fold_maes.append(mae)\n",
    "                fold_r2s.append(r2)\n",
    "                fold_mapes.append(mape_val)\n",
    "\n",
    "            master_results.append({\n",
    "                \"Feature\": feature_name,\n",
    "                \"Model\": \"XGBoost\",\n",
    "                \"Config\": config['name'],\n",
    "                \"MAE\": np.mean(fold_maes),\n",
    "                \"R2\": np.mean(fold_r2s),\n",
    "                \"Samples_Used\": len(y_curr),\n",
    "                \"Error_%\": np.mean(fold_mapes)\n",
    "            })\n",
    "            \n",
    "            # Save Checkpoint\n",
    "            pd.DataFrame(master_results).to_csv(save_file, index=False)\n",
    "\n",
    "    return pd.DataFrame(master_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b01005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running {'objective': 'reg:squarederror', 'n_estimators': 300, 'learning_rate': 0.05, 'max_depth': 3, 'subsample': 1, 'colsample_bytree': 1, 'n_jobs': -1, 'random_state': 42}:   0%|          | 0/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown model_type: {'objective': 'reg:squarederror', 'n_estimators': 300, 'learning_rate': 0.05, 'max_depth': 3, 'subsample': 1, 'colsample_bytree': 1, 'n_jobs': -1, 'random_state': 42}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      2\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m300\u001b[39m, \n\u001b[0;32m      3\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.05\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m      9\u001b[0m                     }\n\u001b[1;32m---> 13\u001b[0m xgboost_4_result \u001b[38;5;241m=\u001b[39m run_modular_experiment(\n\u001b[0;32m     14\u001b[0m     feature_vals,\n\u001b[0;32m     15\u001b[0m     df_spectra_snv\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m     16\u001b[0m     ph_scaled,\n\u001b[0;32m     17\u001b[0m     ec_scaled,\n\u001b[0;32m     18\u001b[0m         params,\n\u001b[0;32m     19\u001b[0m     save_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgboost_dynamic_results(depth = 3(clay)).csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m, in \u001b[0;36mrun_modular_experiment\u001b[1;34m(target_dict, X_spectral, X_ph, X_ec, model_type, save_file, **model_params)\u001b[0m\n\u001b[0;32m     29\u001b[0m fold_maes, fold_r2s, fold_mapes \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, val_idx \u001b[38;5;129;01min\u001b[39;00m kf\u001b[38;5;241m.\u001b[39msplit(X_s_curr):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Call the worker helper\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     mae, r2, mape \u001b[38;5;241m=\u001b[39m process_single_fold(\n\u001b[0;32m     34\u001b[0m         X_s_curr, y_curr, X_p_curr, X_e_curr, \n\u001b[0;32m     35\u001b[0m         train_idx, val_idx, \n\u001b[0;32m     36\u001b[0m         config, model_type, model_params\n\u001b[0;32m     37\u001b[0m     )\n\u001b[0;32m     39\u001b[0m     fold_maes\u001b[38;5;241m.\u001b[39mappend(mae)\n\u001b[0;32m     40\u001b[0m     fold_r2s\u001b[38;5;241m.\u001b[39mappend(r2)\n",
      "Cell \u001b[1;32mIn[12], line 103\u001b[0m, in \u001b[0;36mprocess_single_fold\u001b[1;34m(X_s, y, X_p, X_e, train_idx, val_idx, config, model_type, params)\u001b[0m\n\u001b[0;32m     98\u001b[0m x_train_s, y_train, x_train_p, x_train_e \u001b[38;5;241m=\u001b[39m augment_all_data(\n\u001b[0;32m     99\u001b[0m     x_train_s, y_train, x_train_p, x_train_e, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[0;32m    100\u001b[0m )\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# 3. Format Inputs (Stacking vs List)\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m train_input \u001b[38;5;241m=\u001b[39m format_inputs(x_train_s, x_train_p, x_train_e, config, model_type)\n\u001b[0;32m    104\u001b[0m val_input   \u001b[38;5;241m=\u001b[39m format_inputs(x_val_s, x_val_p, x_val_e, config, model_type)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# 4. Train & Predict\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 43\u001b[0m, in \u001b[0;36mformat_inputs\u001b[1;34m(x_s, x_p, x_e, config, model_type)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_list\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown model_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown model_type: {'objective': 'reg:squarederror', 'n_estimators': 300, 'learning_rate': 0.05, 'max_depth': 3, 'subsample': 1, 'colsample_bytree': 1, 'n_jobs': -1, 'random_state': 42}"
     ]
    }
   ],
   "source": [
    "params = {\"objective\":'reg:squarederror',\n",
    "                    \"n_estimators\":300, \n",
    "                    \"learning_rate\":0.05, \n",
    "                    \"max_depth\":3,\n",
    "                    \"subsample\":1, \n",
    "                    \"colsample_bytree\":1, \n",
    "                    \"n_jobs\":-1, \n",
    "                    \"random_state\":42\n",
    "                    }\n",
    "\n",
    "\n",
    "\n",
    "xgboost_4_result = run_modular_experiment(\n",
    "    feature_vals,\n",
    "    df_spectra_snv.values,\n",
    "    ph_scaled,\n",
    "    ec_scaled,\n",
    "        params,\n",
    "    save_file=\"xgboost_dynamic_results(depth = 3(clay)).csv\"\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08c1cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(feature_vals)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_vals' is not defined"
     ]
    }
   ],
   "source": [
    "print(feature_vals)\n",
    "#xgboost_6_result = run_dynamic_xgboost_experiment(feature_vals, df_spectra_snv.values, ph_scaled, ec_scaled, 6, \"xgboost_dynamic_results(depth = 6).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#xgboost_8_result = run_dynamic_xgboost_experiment(feature_vals, df_spectra_snv.values, ph_scaled, ec_scaled, 8, \"xgboost_dynamic_results(depth = 8).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the experiment (reduced neurons)\n",
    "#cnn_df = run_multi_feature_experiment(feature_vals, df_spectra_snv.values, ph_scaled, ec_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4719f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncnn_result = pd.read_csv(\"experiment_results_checkpoint.csv\")\\n\\nprint(cnn_result[cnn_result[\"Config\"] == \"Spectral + pH + EC\"].sort_values(by=\"MAE\"))\\nprint(\"\\n\\n\")\\nprint(cnn_result[cnn_result[\"Config\"] == \"Spectral + pH\"].sort_values(by=\"MAE\"))\\nprint(\"\\n\\n\")\\nprint(cnn_result[cnn_result[\"Config\"] == \"Spectral + EC\"].sort_values(by=\"MAE\"))\\nprint(\"\\n\\n\")\\nprint(cnn_result[cnn_result[\"Config\"] == \"Spectral Only\"].sort_values(by=\"MAE\"))'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cnn_result = pd.read_csv(\"experiment_results_checkpoint.csv\")\n",
    "\n",
    "print(cnn_result[cnn_result[\"Config\"] == \"Spectral + pH + EC\"].sort_values(by=\"MAE\"))\n",
    "print(\"\\n\\n\")\n",
    "print(cnn_result[cnn_result[\"Config\"] == \"Spectral + pH\"].sort_values(by=\"MAE\"))\n",
    "print(\"\\n\\n\")\n",
    "print(cnn_result[cnn_result[\"Config\"] == \"Spectral + EC\"].sort_values(by=\"MAE\"))\n",
    "print(\"\\n\\n\")\n",
    "print(cnn_result[cnn_result[\"Config\"] == \"Spectral Only\"].sort_values(by=\"MAE\"))'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
