{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a110b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\jagda\\anaconda3\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jagda\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\jagda\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jagda\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: pillow in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Downloading xgboost-3.1.2-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/72.0 MB 3.5 MB/s eta 0:00:21\n",
      "    --------------------------------------- 1.6/72.0 MB 4.5 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 2.6/72.0 MB 4.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 3.4/72.0 MB 5.2 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 5.0/72.0 MB 5.1 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 6.3/72.0 MB 5.4 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 7.6/72.0 MB 5.6 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 8.9/72.0 MB 5.7 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 10.2/72.0 MB 5.8 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 11.5/72.0 MB 5.8 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 12.3/72.0 MB 5.7 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 13.1/72.0 MB 5.5 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 13.9/72.0 MB 5.3 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 14.7/72.0 MB 5.2 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 15.2/72.0 MB 5.2 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 15.2/72.0 MB 5.2 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 15.2/72.0 MB 5.2 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 15.2/72.0 MB 5.2 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 15.2/72.0 MB 5.2 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 15.5/72.0 MB 3.8 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 15.5/72.0 MB 3.8 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 15.5/72.0 MB 3.8 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 15.7/72.0 MB 3.3 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 15.7/72.0 MB 3.3 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 15.7/72.0 MB 3.3 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 15.7/72.0 MB 3.3 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 15.7/72.0 MB 3.3 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 15.7/72.0 MB 3.3 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 15.7/72.0 MB 3.3 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 15.7/72.0 MB 3.3 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 15.7/72.0 MB 3.3 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 16.0/72.0 MB 2.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 16.0/72.0 MB 2.5 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 16.3/72.0 MB 2.3 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 16.3/72.0 MB 2.3 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 16.3/72.0 MB 2.3 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 16.3/72.0 MB 2.3 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 16.3/72.0 MB 2.3 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 16.3/72.0 MB 2.3 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 16.3/72.0 MB 2.3 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 16.5/72.0 MB 1.9 MB/s eta 0:00:29\n",
      "   --------- ------------------------------ 16.5/72.0 MB 1.9 MB/s eta 0:00:29\n",
      "   --------- ------------------------------ 16.8/72.0 MB 1.9 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 17.0/72.0 MB 1.9 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 17.6/72.0 MB 1.9 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 17.8/72.0 MB 1.9 MB/s eta 0:00:30\n",
      "   ---------- ----------------------------- 18.4/72.0 MB 1.9 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 18.6/72.0 MB 1.9 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 18.9/72.0 MB 1.9 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 19.4/72.0 MB 1.9 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 19.7/72.0 MB 1.9 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 20.2/72.0 MB 1.9 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 20.2/72.0 MB 1.9 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 20.7/72.0 MB 1.8 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 21.2/72.0 MB 1.8 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 21.8/72.0 MB 1.9 MB/s eta 0:00:27\n",
      "   ------------ --------------------------- 22.5/72.0 MB 1.9 MB/s eta 0:00:27\n",
      "   ------------ --------------------------- 23.1/72.0 MB 1.9 MB/s eta 0:00:26\n",
      "   ------------- -------------------------- 23.9/72.0 MB 1.9 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 24.6/72.0 MB 2.0 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 24.6/72.0 MB 2.0 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 25.2/72.0 MB 1.9 MB/s eta 0:00:25\n",
      "   -------------- ------------------------- 25.7/72.0 MB 2.0 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 26.5/72.0 MB 2.0 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 27.3/72.0 MB 2.0 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 28.0/72.0 MB 2.0 MB/s eta 0:00:22\n",
      "   ---------------- ----------------------- 29.1/72.0 MB 2.1 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 29.9/72.0 MB 2.1 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 30.9/72.0 MB 2.1 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 32.0/72.0 MB 2.2 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 32.8/72.0 MB 2.2 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 33.6/72.0 MB 2.2 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 34.1/72.0 MB 2.2 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 34.6/72.0 MB 2.2 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 35.1/72.0 MB 2.3 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 35.1/72.0 MB 2.3 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 35.1/72.0 MB 2.3 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 35.1/72.0 MB 2.3 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 35.4/72.0 MB 2.2 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 35.4/72.0 MB 2.2 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 35.7/72.0 MB 2.1 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 35.9/72.0 MB 2.1 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 35.9/72.0 MB 2.1 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 36.2/72.0 MB 2.1 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 36.2/72.0 MB 2.1 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 36.2/72.0 MB 2.1 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 36.2/72.0 MB 2.1 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 36.7/72.0 MB 2.0 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 37.5/72.0 MB 2.0 MB/s eta 0:00:18\n",
      "   --------------------- ------------------ 38.5/72.0 MB 2.1 MB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 39.8/72.0 MB 2.1 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 41.2/72.0 MB 2.2 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 42.5/72.0 MB 2.2 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 44.0/72.0 MB 2.2 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 45.6/72.0 MB 2.3 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 47.2/72.0 MB 2.4 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 48.8/72.0 MB 2.4 MB/s eta 0:00:10\n",
      "   --------------------------- ------------ 49.5/72.0 MB 2.4 MB/s eta 0:00:10\n",
      "   --------------------------- ------------ 50.3/72.0 MB 2.4 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 51.4/72.0 MB 2.5 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 51.4/72.0 MB 2.5 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 51.9/72.0 MB 2.4 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 52.7/72.0 MB 2.5 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 53.2/72.0 MB 2.5 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 53.7/72.0 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 54.0/72.0 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 54.0/72.0 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 54.0/72.0 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 54.3/72.0 MB 2.4 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 55.1/72.0 MB 2.4 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 55.6/72.0 MB 2.4 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 56.4/72.0 MB 2.4 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 57.1/72.0 MB 2.4 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 57.9/72.0 MB 2.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 59.0/72.0 MB 2.5 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 59.5/72.0 MB 2.5 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 60.3/72.0 MB 2.5 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 61.1/72.0 MB 2.5 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 61.9/72.0 MB 2.5 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 62.7/72.0 MB 2.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 63.7/72.0 MB 2.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 64.7/72.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 65.5/72.0 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 66.6/72.0 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 67.6/72.0 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 68.7/72.0 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 70.0/72.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.0/72.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Docstring for models.ipynb\n",
    "using spectral_feature_data.csv to train ML models and check their performance\n",
    "the training and testing split is 80% - 20%\n",
    "training is done using cross validation with 5 folds\n",
    "models going to be used:\n",
    "- Random Forest regressor\n",
    "- XGBoost regressor\n",
    "- CNN\n",
    "\n",
    "'''\n",
    "%pip install tensorflow scikit-learn pandas numpy tensorflow xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras import models, layers, Input\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import xgboost as xgb\n",
    "\n",
    "raw_features = pd.read_csv('spectral_feature_data.csv')\n",
    "raw_features.drop([\"p3.Fe.mg_kg\", \"p2.Zn.mg_kg\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING EASY, MEDIUM, HARD FEATURES BASED ON OCCURANCE\n",
    "total_rows = raw_features.shape[0]\n",
    "df = pd.DataFrame()\n",
    "print(raw_features.shape)\n",
    "df_p_cols = [col for col in raw_features.columns if col.startswith('p')]\n",
    "easy_features = []\n",
    "print(\"Total rows in most occurance columns (>40k):\")\n",
    "for col in df_p_cols:\n",
    "    if raw_features[col].isna().sum() < 10000:\n",
    "        print(col,\"\\t\\t\", total_rows - raw_features[col].isna().sum())\n",
    "        easy_features.append(col)\n",
    "\n",
    "print(\"\\n\")\n",
    "medium_features = [col for col in df_p_cols if col not in easy_features and raw_features[col].notna().sum() > 20000]\n",
    "print(\"Total Rows in medium occurance columns (20k-40k):\")\n",
    "print(raw_features[medium_features].notna().sum())\n",
    "hard_features = [col for col in df_p_cols if col not in easy_features and col not in medium_features]\n",
    "print(\"\\n\")\n",
    "print(\"Total Rows in hard occurance columns (<20k):\") \n",
    "print(raw_features[hard_features].notna().sum())\n",
    "spectral_cols = [col for col in raw_features.columns if col[0] != 'p']\n",
    "print(f\"\\nSpectral Wavelengths: (No. of readings: {len(spectral_cols)})\\n\\n{spectral_cols}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(medium_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROW-WISE STANDARDIZATION (Standard Normal Variate - SNV) FOR SPECTRA\n",
    "# ======================================================\n",
    "def apply_snv(input_data):\n",
    "    \"\"\"\n",
    "    Standard Normal Variate (SNV):\n",
    "    Subtracts the Row Mean and divides by Row Std Dev.\n",
    "    \"\"\"\n",
    "    # axis=1 means \"calculate across the row\"\n",
    "    row_mean = input_data.mean(axis=1)\n",
    "    row_std = input_data.std(axis=1)\n",
    "    \n",
    "    # We use numpy broadcasting to subtract/divide\n",
    "    # (data - mean) / std\n",
    "    snv_data = (input_data.sub(row_mean, axis=0)).div(row_std, axis=0)\n",
    "    return snv_data\n",
    "\n",
    "# Apply only to spectral columns\n",
    "df_spectra_snv = apply_snv(raw_features[spectral_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e673902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to synthesize data by adding noise to existing data points\n",
    "#helps to increase dataset size for better training of models especially deep learning models like CNNs\n",
    "def augment_all_data(X_s, y, X_ph=None, X_ec=None, min_samples=500):\n",
    "    \"\"\"\n",
    "    Augments Spectral data + pH + EC with sensor-specific noise profiles.\n",
    "    \"\"\"\n",
    "    n_samples = X_s.shape[0]\n",
    "    \n",
    "    # 1. Skip if we have enough data\n",
    "    if n_samples >= min_samples:\n",
    "        return X_s, y, X_ph, X_ec\n",
    "        \n",
    "    # --- NOISE GENERATION ---\n",
    "    \n",
    "    # A. Spectral Noise (Background electronic noise)\n",
    "    # 1% of the standard deviation of the spectra\n",
    "    spec_noise_level = 0.01 * np.std(X_s) \n",
    "    noise_s = np.random.normal(0, spec_noise_level, X_s.shape)\n",
    "    \n",
    "    # B. pH Noise (Absolute Error)\n",
    "    # Standard pH probe accuracy is often +/- 0.02 to 0.05\n",
    "    if X_ph is not None:\n",
    "        noise_ph = np.random.normal(0, 0.05, X_ph.shape) # 0.05 pH unit variance\n",
    "    \n",
    "    # C. EC Noise (Relative Error)\n",
    "    # EC sensors usually have % error (e.g. 2%)\n",
    "    if X_ec is not None:\n",
    "        # Noise is 2% of the actual reading value\n",
    "        noise_ec = X_ec * np.random.normal(0, 0.02, X_ec.shape)\n",
    "\n",
    "    # --- APPLY & CONCATENATE ---\n",
    "    \n",
    "    # Create Noisy Copies\n",
    "    X_s_new  = X_s + noise_s\n",
    "    X_ph_new = (X_ph + noise_ph) if X_ph is not None else None\n",
    "    X_ec_new = (X_ec + noise_ec) if X_ec is not None else None\n",
    "    \n",
    "    # Concatenate (Original + Noisy)\n",
    "    X_s_aug = np.concatenate([X_s, X_s_new], axis=0)\n",
    "    y_aug   = np.concatenate([y, y], axis=0)\n",
    "    \n",
    "    X_ph_aug = np.concatenate([X_ph, X_ph_new], axis=0) if X_ph is not None else None\n",
    "    X_ec_aug = np.concatenate([X_ec, X_ec_new], axis=0) if X_ec is not None else None\n",
    "    \n",
    "    return X_s_aug, y_aug, X_ph_aug, X_ec_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f04d62e",
   "metadata": {},
   "source": [
    "For training the data, we will use a cnn on the easy features directly getting a 1D vector as output.\\n\n",
    "For the medium and hard data we will train individual models per feature getting a 0D output. \\n\n",
    "We will try using xgboost, and CNNs to see which model gives the best. \\n\n",
    "Final output will be a vector concatenating all of the outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_flexible_model(input_shape, use_ph=False, use_ec=False):\n",
    "    \"\"\"\n",
    "    Builds a model with optional pH and EC inputs.\n",
    "    \"\"\"\n",
    "    inputs_list = []\n",
    "    features_to_concat = []\n",
    "\n",
    "    # --- 1. Spectral Branch (Always Present) ---\n",
    "    input_spec = Input(shape=input_shape, name='spectral_in')\n",
    "    inputs_list.append(input_spec)\n",
    "    \n",
    "    # Feature extraction logic\n",
    "    x = layers.Conv1D(16, 3, activation='relu')(input_spec)\n",
    "    x = layers.Conv1D(8, 3, activation='relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    features_to_concat.append(x)\n",
    "\n",
    "    # --- 2. Conditional pH Branch ---\n",
    "    if use_ph:\n",
    "        input_ph = Input(shape=(1,), name='ph_in')\n",
    "        inputs_list.append(input_ph)\n",
    "        features_to_concat.append(input_ph)\n",
    "\n",
    "    # --- 3. Conditional EC Branch ---\n",
    "    if use_ec:\n",
    "        input_ec = Input(shape=(1,), name='ec_in')\n",
    "        inputs_list.append(input_ec)\n",
    "        features_to_concat.append(input_ec)\n",
    "\n",
    "    # --- 4. Merge & Output ---\n",
    "    # If we have more than one feature source, we concatenate\n",
    "    if len(features_to_concat) > 1:\n",
    "        x = layers.Concatenate()(features_to_concat)\n",
    "    else:\n",
    "        x = features_to_concat[0] # Just the spectral features\n",
    "\n",
    "    # Regression Head\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(8, activation='relu')(x)\n",
    "    output = layers.Dense(1, name='output')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs_list, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42350582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining dataset, Scaling the values\n",
    "# dataset contains: \n",
    "# 1. scaled spectral columns\n",
    "# 2. ph and ec columns\n",
    "# 3. other target features\n",
    "\n",
    "features = easy_features + medium_features + hard_features\n",
    "df_final = pd.concat([df_spectra_snv, raw_features[features]], axis=1) #has snv spectral with non scaled other features\n",
    "print(df_final.shape)\n",
    "\n",
    "#these are the scaled versions of ph and ec to be used as inputs when required\n",
    "scaler = StandardScaler()\n",
    "ec_scaled = scaler.fit_transform(raw_features[[\"p1.EC.ds_m\"]])\n",
    "ph_scaled = scaler.fit_transform(raw_features[[\"p1.pH.index\"]])\n",
    "\n",
    "feature_vals = {}\n",
    "for feature in features:\n",
    "    feature_vals[feature] = df_final[feature].values\n",
    "    \n",
    "print(feature_vals)\n",
    "print(ec_scaled, ph_scaled)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_multi_feature_experiment(data_dictionary, X_spectral, X_ph, X_ec):\n",
    "    \"\"\"\n",
    "    data_dictionary: {'Nitrogen': y_nitrogen_array, 'Phosphorus': y_phosphorus_array, ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Setup Storage\n",
    "    master_results = []\n",
    "    \n",
    "    # 2. Define Configs\n",
    "    configs = [\n",
    "        {\"name\": \"Spectral Only\",      \"ph\": False, \"ec\": False},\n",
    "        {\"name\": \"Spectral + pH\",      \"ph\": True,  \"ec\": False},\n",
    "        {\"name\": \"Spectral + EC\",      \"ph\": False, \"ec\": True},\n",
    "        {\"name\": \"Spectral + pH + EC\", \"ph\": True,  \"ec\": True}\n",
    "    ]\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    X_ph_temp = X_ph.copy() # Make a copy so we don't change original data\n",
    "    X_ph_temp[X_ph_temp == 0] = np.nan\n",
    "\n",
    "    # If X_ec has 0s that act as NaNs:\n",
    "    X_ec_temp = X_ec.copy()\n",
    "    X_ec_temp[X_ec_temp == 0] = np.nan\n",
    "\n",
    "    \n",
    "    # 3. Outer Loop: Iterate through Features (Nitrogen, Potassium, etc.)\n",
    "    # tqdm makes a nice progress bar\n",
    "    for feature_name, y_full in tqdm(data_dictionary.items(), desc=\"Processing Features\"):\n",
    "        \n",
    "        print(f\"\\n--- Starting Feature: {feature_name} ---\")\n",
    "        # --- 2. Create Intelligent Mask for NaN Removal ---\n",
    "        # We keep a row ONLY if: Target is valid AND pH is valid AND EC is valid\n",
    "        # This ensures all 4 configs train on the exact same dataset for fair comparison.\n",
    "        \n",
    "        # Check where data exists (True if valid, False if NaN)\n",
    "        # Assuming inputs are 1D arrays or flattened. \n",
    "        # If X_ph/X_ec are (N,1) shape, we use .flatten() just for the check\n",
    "        \n",
    "        y_full[y_full == 0] = np.nan \n",
    "        \n",
    "        # If X_ph has 0s that act as NaNs:\n",
    "\n",
    "        \n",
    "        # --- 2. Create Intelligent Mask (Same as before) ---\n",
    "        valid_y  = ~np.isnan(y_full).flatten()\n",
    "        valid_ph = ~np.isnan(X_ph_temp).flatten() # Use the temp version\n",
    "        valid_ec = ~np.isnan(X_ec_temp).flatten() # Use the temp version\n",
    "        \n",
    "\n",
    "        printed = False  # To ensure we print dropped rows info only once per feature\n",
    "        # 4. Middle Loop: Configurations\n",
    "        for config in configs:\n",
    "            \n",
    "            fold_maes = []\n",
    "            fold_r2s = []\n",
    "            fold_mapes = []\n",
    "\n",
    "            mask = valid_y.copy() # Good practice to copy, though not strictly required\n",
    "            \n",
    "            # 2. Add pH requirement ONLY if the config needs it\n",
    "            if config['ph']:\n",
    "                mask = mask & valid_ph\n",
    "                \n",
    "            # 3. Add EC requirement ONLY if the config needs it\n",
    "            if config['ec']:\n",
    "                mask = mask & valid_ec\n",
    "        \n",
    "            # Apply Mask to Create \"Clean\" Subsets for this Feature\n",
    "            X_s_clean = X_spectral[mask]\n",
    "            X_p_clean = X_ph_temp[mask]\n",
    "            X_e_clean = X_ec_temp[mask]\n",
    "            y_clean   = y_full[mask]\n",
    "\n",
    "            n_removed = len(y_full) - len(y_clean)\n",
    "            if n_removed > 0:\n",
    "                    # Optional: Print info if data was dropped\n",
    "                tqdm.write(f\"  > Feature '{feature_name}': Dropped {n_removed} rows due to NaNs.\")\n",
    "\n",
    "                # If data is empty after cleaning, skip\n",
    "            if len(y_clean) < 300: \n",
    "                print(f\"  ! Skipping {feature_name}: Not enough data points after cleaning.\")\n",
    "                break\n",
    "                \n",
    "            # 5. Inner Loop: 5-Fold Cross Validation\n",
    "            if len(X_s_clean) < 10:\n",
    "                continue\n",
    "\n",
    "            for train_idx, val_idx in kf.split(X_s_clean):\n",
    "                \n",
    "                # --- STEP 1: EXTRACT RAW DATA ---\n",
    "                x_train_s, x_val_s = X_s_clean[train_idx], X_s_clean[val_idx]\n",
    "                y_train, y_val     = y_clean[train_idx],   y_clean[val_idx]\n",
    "                \n",
    "                # Extract pH (or set to None if not used)\n",
    "                if config['ph']:\n",
    "                    x_train_p = X_p_clean[train_idx]\n",
    "                    x_val_p   = X_p_clean[val_idx]\n",
    "                else:\n",
    "                    x_train_p, x_val_p = None, None\n",
    "\n",
    "                # Extract EC (or set to None if not used)\n",
    "                if config['ec']:\n",
    "                    x_train_e = X_e_clean[train_idx]\n",
    "                    x_val_e   = X_e_clean[val_idx]\n",
    "                else:\n",
    "                    x_train_e, x_val_e = None, None\n",
    "\n",
    "                # --- STEP 2: AUGMENTATION (Modify the variables) ---\n",
    "                # This updates x_train_s, y_train, etc. to include the noisy copies\n",
    "                x_train_s, y_train, x_train_p, x_train_e = augment_all_data(\n",
    "                    x_train_s, y_train, x_train_p, x_train_e, min_samples=500\n",
    "                )\n",
    "\n",
    "                # --- STEP 3: BUILD INPUT LISTS (Using the NEW augmented variables) ---\n",
    "                \n",
    "                # Training Inputs (Augmented)\n",
    "                train_inputs = [x_train_s]\n",
    "                if config['ph']:\n",
    "                    train_inputs.append(x_train_p)\n",
    "                if config['ec']:\n",
    "                    train_inputs.append(x_train_e)\n",
    "                    \n",
    "                # Validation Inputs (Original - never augmented)\n",
    "                val_inputs = [x_val_s]\n",
    "                if config['ph']:\n",
    "                    val_inputs.append(x_val_p)\n",
    "                if config['ec']:\n",
    "                    val_inputs.append(x_val_e)\n",
    "\n",
    "\n",
    "                # B. Build & Train\n",
    "                # (Make sure build_flexible_model is defined in your scope)\n",
    "                model = build_flexible_model(\n",
    "                    input_shape=(X_s_clean.shape[1], 1),\n",
    "                    use_ph=config['ph'],\n",
    "                    use_ec=config['ec']\n",
    "                )\n",
    "                \n",
    "                model.fit(\n",
    "                    x=train_inputs, y=y_train,\n",
    "                    validation_data=(val_inputs, y_val),\n",
    "                    epochs=30, batch_size=32, verbose=0\n",
    "                )\n",
    "                \n",
    "                # C. Evaluate\n",
    "                preds = model.predict(val_inputs, verbose=0).flatten()\n",
    "                fold_maes.append(mean_absolute_error(y_val, preds))\n",
    "                fold_r2s.append(r2_score(y_val, preds))\n",
    "                fold_mapes.append(mape_val)\n",
    "            \n",
    "            # 6. Aggregate Results for this Config\n",
    "            avg_mae = np.mean(fold_maes)\n",
    "            avg_r2 = np.mean(fold_r2s)\n",
    "            \n",
    "            \n",
    "            # Add to list\n",
    "            master_results.append({\n",
    "                \"Feature\": feature_name,\n",
    "                \"Config\": config['name'],\n",
    "                \"MAE\": avg_mae,\n",
    "                \"R2\": avg_r2,\n",
    "                \"Samples_Used\": len(y_clean)\n",
    "                \"Error_%\": np.mean(fold_mapes)\n",
    "            })\n",
    "            \n",
    "         # 7. SAVE CHECKPOINT (Crucial for long runs)\n",
    "         # Saves to CSV after every feature is done, so you never lose work.\n",
    "        df_temp = pd.DataFrame(master_results)\n",
    "        df_temp.to_csv(\"experiment_results_checkpoint(reduced neurons).csv\", index=False)\n",
    "        print(f\"Saved checkpoint for {feature_name}\")\n",
    "\n",
    "    return pd.DataFrame(master_results)\n",
    "\n",
    "# --- How to Run It ---\n",
    "\n",
    "# 1. Create a dictionary of your targets\n",
    "# y_targets = {\n",
    "#     \"Nitrogen\": y_n,\n",
    "#     \"Phosphorus\": y_p,\n",
    "#     \"Potassium\": y_k,\n",
    "#     # ... add all 16 features here\n",
    "# }\n",
    "\n",
    "# 2. Run\n",
    "# final_df = run_multi_feature_experiment(y_targets, X_s_clean, X_ph, X_ec)\n",
    "# print(final_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dynamic_xgboost_experiment(target_dict, X_spectral, X_ph, X_ec, depth, save_file):\n",
    "    master_results = []\n",
    "    \n",
    "    configs = [\n",
    "        {\"name\": \"Spectral Only\",      \"ph\": False, \"ec\": False},\n",
    "        {\"name\": \"Spectral + pH\",      \"ph\": True,  \"ec\": False},\n",
    "        {\"name\": \"Spectral + EC\",      \"ph\": False, \"ec\": True},\n",
    "        {\"name\": \"Spectral + pH + EC\", \"ph\": True,  \"ec\": True}\n",
    "    ]\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for feature_name, y_full in tqdm(target_dict.items(), desc=\"XGBoost Features\"):\n",
    "        \n",
    "        # Base Valid Mask (Target must exist)\n",
    "        base_valid_y = ~np.isnan(y_full).flatten()\n",
    "        \n",
    "        # Temp copies for masking checks\n",
    "        X_ph_temp = X_ph.copy()\n",
    "        X_ec_temp = X_ec.copy()\n",
    "        valid_ph = ~np.isnan(X_ph_temp).flatten()\n",
    "        valid_ec = ~np.isnan(X_ec_temp).flatten()\n",
    "        \n",
    "        for config in configs:\n",
    "            fold_maes = []\n",
    "            fold_r2s  = []\n",
    "            fold_mapes = []\n",
    "            \n",
    "            # 1. Build Dynamic Mask\n",
    "            mask = base_valid_y.copy()\n",
    "            if config['ph']: mask = mask & valid_ph\n",
    "            if config['ec']: mask = mask & valid_ec\n",
    "            \n",
    "            # 2. Extract CLEAN subsets\n",
    "            X_s_curr = X_spectral[mask]\n",
    "            y_curr   = y_full[mask]\n",
    "            X_p_curr = X_ph_temp[mask]\n",
    "            X_e_curr = X_ec_temp[mask]\n",
    "            \n",
    "            if len(y_curr) < 10: continue\n",
    "\n",
    "            # --- START CV LOOP ---\n",
    "            for train_idx, val_idx in kf.split(X_s_curr):\n",
    "                \n",
    "                # A. EXTRACT Raw Components\n",
    "                x_train_s, x_val_s = X_s_curr[train_idx], X_s_curr[val_idx]\n",
    "                y_train, y_val     = y_curr[train_idx],   y_curr[val_idx]\n",
    "                \n",
    "                # Handle optional inputs\n",
    "                if config['ph']:\n",
    "                    x_train_p, x_val_p = X_p_curr[train_idx], X_p_curr[val_idx]\n",
    "                else:\n",
    "                    x_train_p, x_val_p = None, None\n",
    "                    \n",
    "                if config['ec']:\n",
    "                    x_train_e, x_val_e = X_e_curr[train_idx], X_e_curr[val_idx]\n",
    "                else:\n",
    "                    x_train_e, x_val_e = None, None\n",
    "\n",
    "                # B. AUGMENTATION (Training Data Only)\n",
    "                x_train_s, y_train, x_train_p, x_train_e = augment_all_data(\n",
    "                    x_train_s, y_train, x_train_p, x_train_e, min_samples=500\n",
    "                )\n",
    "                \n",
    "                # C. STACKING (Build Matrices)\n",
    "                \n",
    "                # 1. Training Matrix (Augmented)\n",
    "                X_train_final = x_train_s\n",
    "                if config['ph']:\n",
    "                    X_train_final = np.hstack([X_train_final, x_train_p.reshape(-1, 1)])\n",
    "                if config['ec']:\n",
    "                    X_train_final = np.hstack([X_train_final, x_train_e.reshape(-1, 1)])\n",
    "                    \n",
    "                # 2. Validation Matrix (Original)\n",
    "                X_val_final = x_val_s\n",
    "                if config['ph']:\n",
    "                    X_val_final = np.hstack([X_val_final, x_val_p.reshape(-1, 1)])\n",
    "                if config['ec']:\n",
    "                    X_val_final = np.hstack([X_val_final, x_val_e.reshape(-1, 1)])\n",
    "\n",
    "                # D. TRAIN\n",
    "                model = xgb.XGBRegressor(\n",
    "                    objective='reg:squarederror',\n",
    "                    n_estimators=500, learning_rate=0.05, max_depth=depth,\n",
    "                    subsample=0.8, colsample_bytree=0.8, n_jobs=-1, random_state=42\n",
    "                )\n",
    "                \n",
    "                model.fit(\n",
    "                    X_train_final, y_train,\n",
    "                    eval_set=[(X_val_final, y_val)],\n",
    "                    early_stopping_rounds=20, verbose=False\n",
    "                )\n",
    "                \n",
    "                # E. EVALUATE (Corrected Order)\n",
    "                # 1. Predict FIRST\n",
    "                preds = model.predict(X_val_final)\n",
    "                \n",
    "                # 2. Calculate Metrics SECOND\n",
    "                mae = mean_absolute_error(y_val, preds)\n",
    "                r2  = r2_score(y_val, preds)\n",
    "                # Safe MAPE calculation\n",
    "                mape_val = np.mean(np.abs((y_val - preds) / (y_val + 1e-6))) * 100\n",
    "\n",
    "                fold_maes.append(mae)\n",
    "                fold_r2s.append(r2)\n",
    "                fold_mapes.append(mape_val)\n",
    "\n",
    "            master_results.append({\n",
    "                \"Feature\": feature_name,\n",
    "                \"Model\": \"XGBoost\",\n",
    "                \"Config\": config['name'],\n",
    "                \"MAE\": np.mean(fold_maes),\n",
    "                \"R2\": np.mean(fold_r2s),\n",
    "                \"Samples_Used\": len(y_curr),\n",
    "                \"Error_%\": np.mean(fold_mapes)\n",
    "            })\n",
    "            \n",
    "            # Save Checkpoint\n",
    "            pd.DataFrame(master_results).to_csv(save_file, index=False)\n",
    "\n",
    "    return pd.DataFrame(master_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_dynamic_xgboost_experiment(feature_vals, df_spectra_snv.values, ph_scaled, ec_scaled, 6, \"xgboost_dynamic_results(depth = 6).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_dynamic_xgboost_experiment(feature_vals, df_spectra_snv.values, ph_scaled, ec_scaled, 8, \"xgboost_dynamic_results(depth = 8).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the experiment (reduced neurons)\n",
    "final_df = run_multi_feature_experiment(feature_vals, df_spectra_snv.values, ph_scaled, ec_scaled)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4719f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_result = pd.read_csv(\"experiment_results_checkpoint.csv\")\n",
    "\n",
    "print(cnn_result[cnn_result[\"Config\"] == \"Spectral + pH + EC\"].sort_values(by=\"MAE\"))\n",
    "print(\"\\n\\n\")\n",
    "print(cnn_result[cnn_result[\"Config\"] == \"Spectral + pH\"].sort_values(by=\"MAE\"))\n",
    "print(\"\\n\\n\")\n",
    "print(cnn_result[cnn_result[\"Config\"] == \"Spectral + EC\"].sort_values(by=\"MAE\"))\n",
    "print(\"\\n\\n\")\n",
    "print(cnn_result[cnn_result[\"Config\"] == \"Spectral Only\"].sort_values(by=\"MAE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb169786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c71a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
