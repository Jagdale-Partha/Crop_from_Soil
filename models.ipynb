{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a110b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\jagda\\anaconda3\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jagda\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\jagda\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jagda\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: pillow in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jagda\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Docstring for models.ipynb\n",
    "using spectral_feature_data.csv to train ML models and check their performance\n",
    "the training and testing split is 80% - 20%\n",
    "training is done using cross validation with 5 folds\n",
    "models going to be used:\n",
    "- Random Forest regressor\n",
    "- XGBoost regressor\n",
    "- CNN\n",
    "\n",
    "'''\n",
    "%pip install tensorflow scikit-learn pandas numpy tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras import models, layers, Input\n",
    "\n",
    "raw_features = pd.read_csv('spectral_feature_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca7e8aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44736, 36)\n",
      "Total rows in most occurance columns (>40k):\n",
      "p1.pH.index \t\t 44590\n",
      "p2.N.wt_pct \t\t 40931\n",
      "p2.OC.wt_pct \t\t 44624\n",
      "p3.K.mg_kg \t\t 44488\n",
      "p3.P.mg_kg \t\t 40764\n",
      "\n",
      "\n",
      "Total Rows in medium occurance columns (20k-40k):\n",
      "p1.EC.ds_m         21833\n",
      "p1.Sand.wt_pct     27139\n",
      "p4.CEC.cmolc_kg    22685\n",
      "p4.CF.wt_pct       23237\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Total Rows in hard occurance columns (<20k):\n",
      "p1.Clay.wt_pct          3915\n",
      "p1.Silt.wt_pct          3901\n",
      "p2.Zn.mg_kg                0\n",
      "p3.Fe.mg_kg                0\n",
      "p3.S.wt_pct              167\n",
      "p4.BD.g_cm3             1162\n",
      "p4.WR_10kPa.wt_pct       924\n",
      "p4.WR_1500kPa.wt_pct     967\n",
      "p4.WR_33kPa.wt_pct       919\n",
      "dtype: int64\n",
      "\n",
      "Spectral Wavelengths: (No. of readings: 18)\n",
      "\n",
      "['410', '435', '460', '485', '510', '535', '560', '585', '610', '645', '680', '705', '730', '760', '810', '860', '900', '940']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DEFINING EASY, MEDIUM, HARD FEATURES BASED ON OCCURANCE\n",
    "total_rows = raw_features.shape[0]\n",
    "df = pd.DataFrame()\n",
    "print(raw_features.shape)\n",
    "df_p_cols = [col for col in raw_features.columns if col.startswith('p')]\n",
    "easy_features = []\n",
    "print(\"Total rows in most occurance columns (>40k):\")\n",
    "for col in df_p_cols:\n",
    "    if raw_features[col].isna().sum() < 10000:\n",
    "        print(col,\"\\t\\t\", total_rows - raw_features[col].isna().sum())\n",
    "        easy_features.append(col)\n",
    "\n",
    "print(\"\\n\")\n",
    "medium_features = [col for col in df_p_cols if col not in easy_features and raw_features[col].notna().sum() > 20000]\n",
    "print(\"Total Rows in medium occurance columns (20k-40k):\")\n",
    "print(raw_features[medium_features].notna().sum())\n",
    "hard_features = [col for col in df_p_cols if col not in easy_features and col not in medium_features]\n",
    "print(\"\\n\")\n",
    "print(\"Total Rows in hard occurance columns (<20k):\") \n",
    "print(raw_features[hard_features].notna().sum())\n",
    "spectral_cols = [col for col in raw_features.columns if col[0] != 'p']\n",
    "print(f\"\\nSpectral Wavelengths: (No. of readings: {len(spectral_cols)})\\n\\n{spectral_cols}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61b5e519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p1.EC.ds_m', 'p1.Sand.wt_pct', 'p4.CEC.cmolc_kg', 'p4.CF.wt_pct']\n"
     ]
    }
   ],
   "source": [
    "print(medium_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffa8c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROW-WISE STANDARDIZATION (Standard Normal Variate - SNV) FOR SPECTRA\n",
    "# ======================================================\n",
    "def apply_snv(input_data):\n",
    "    \"\"\"\n",
    "    Standard Normal Variate (SNV):\n",
    "    Subtracts the Row Mean and divides by Row Std Dev.\n",
    "    \"\"\"\n",
    "    # axis=1 means \"calculate across the row\"\n",
    "    row_mean = input_data.mean(axis=1)\n",
    "    row_std = input_data.std(axis=1)\n",
    "    \n",
    "    # We use numpy broadcasting to subtract/divide\n",
    "    # (data - mean) / std\n",
    "    snv_data = (input_data.sub(row_mean, axis=0)).div(row_std, axis=0)\n",
    "    return snv_data\n",
    "\n",
    "# Apply only to spectral columns\n",
    "df_spectra_snv = apply_snv(raw_features[spectral_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e673902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to synthesize data by adding noise to existing data points\n",
    "#helps to increase dataset size for better training of models especially deep learning models like CNNs\n",
    "def augment_data(X_train, y_train, noise_level=0.01):\n",
    "    \"\"\"\n",
    "    Doubles the dataset by adding a noisy version of every row.\n",
    "    \"\"\"\n",
    "    # Generate Gaussian noise\n",
    "    noise = np.random.normal(0, noise_level, X_train.shape)\n",
    "    \n",
    "    # Create new noisy samples\n",
    "    X_noisy = X_train + noise\n",
    "    \n",
    "    # Combine original + noisy\n",
    "    X_augmented = np.concatenate([X_train, X_noisy], axis=0)\n",
    "    y_augmented = np.concatenate([y_train, y_train], axis=0) # Labels stay the same\n",
    "    \n",
    "    return X_augmented, y_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f04d62e",
   "metadata": {},
   "source": [
    "For training the data, we will use a cnn on the easy features directly getting a 1D vector as output.\\n\n",
    "For the medium and hard data we will train individual models per feature getting a 0D output. \\n\n",
    "We will try using xgboost, and CNNs to see which model gives the best. \\n\n",
    "Final output will be a vector concatenating all of the outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7e4467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_flexible_model(input_shape, use_ph=False, use_ec=False):\n",
    "    \"\"\"\n",
    "    Builds a model with optional pH and EC inputs.\n",
    "    \"\"\"\n",
    "    inputs_list = []\n",
    "    features_to_concat = []\n",
    "\n",
    "    # --- 1. Spectral Branch (Always Present) ---\n",
    "    input_spec = Input(shape=input_shape, name='spectral_in')\n",
    "    inputs_list.append(input_spec)\n",
    "    \n",
    "    # Feature extraction logic\n",
    "    x = layers.Conv1D(32, 3, activation='relu')(input_spec)\n",
    "    x = layers.Conv1D(64, 3, activation='relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    features_to_concat.append(x)\n",
    "\n",
    "    # --- 2. Conditional pH Branch ---\n",
    "    if use_ph:\n",
    "        input_ph = Input(shape=(1,), name='ph_in')\n",
    "        inputs_list.append(input_ph)\n",
    "        features_to_concat.append(input_ph)\n",
    "\n",
    "    # --- 3. Conditional EC Branch ---\n",
    "    if use_ec:\n",
    "        input_ec = Input(shape=(1,), name='ec_in')\n",
    "        inputs_list.append(input_ec)\n",
    "        features_to_concat.append(input_ec)\n",
    "\n",
    "    # --- 4. Merge & Output ---\n",
    "    # If we have more than one feature source, we concatenate\n",
    "    if len(features_to_concat) > 1:\n",
    "        x = layers.Concatenate()(features_to_concat)\n",
    "    else:\n",
    "        x = features_to_concat[0] # Just the spectral features\n",
    "\n",
    "    # Regression Head\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    output = layers.Dense(1, name='output')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs_list, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42350582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "#Defining dataset, Scaling the values\n",
    "# dataset contains: \n",
    "# 1. scaled spectral columns\n",
    "# 2. ph and ec columns\n",
    "# 3. other target features\n",
    "\n",
    "features = easy_features + medium_features + hard_features\n",
    "df_final = pd.concat([df_spectra_snv, raw_features[easy_features + medium_features + hard_features]], axis=1) #has snv spectral with non scaled other features\n",
    "print(df.shape)\n",
    "\n",
    "#these are the scaled versions of ph and ec to be used as inputs when required\n",
    "scaler = StandardScaler()\n",
    "ec_scaled = scaler.fit_transform(raw_features[[\"p1.EC.ds_m\"]])\n",
    "ph_scaled = scaler.fit_transform(raw_features[[\"p1.pH.index\"]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb3e6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_spectral_experiment(X_spectral, X_ph, X_ec, y, feature_name=\"Target\"):\n",
    "    \"\"\"\n",
    "    Runs 5-Fold CV on 4 configurations for a specific target feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n==========================================\")\n",
    "    print(f\" EXPERIMENT: Predicting {feature_name}\")\n",
    "    print(f\"==========================================\\n\")\n",
    "\n",
    "    # Define Configurations\n",
    "    configs = [\n",
    "        {\"name\": \"Spectral Only\",      \"ph\": False, \"ec\": False},\n",
    "        {\"name\": \"Spectral + pH\",      \"ph\": True,  \"ec\": False},\n",
    "        {\"name\": \"Spectral + EC\",      \"ph\": False, \"ec\": True},\n",
    "        {\"name\": \"Spectral + pH + EC\", \"ph\": True,  \"ec\": True}\n",
    "    ]\n",
    "\n",
    "    # Initialize K-Fold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Store aggregated results\n",
    "    summary_results = []\n",
    "\n",
    "    for config in configs:\n",
    "        print(f\">> Configuration: {config['name']}\")\n",
    "        \n",
    "        fold_maes = []\n",
    "        fold_r2s = []\n",
    "        all_preds = [] # To track range across all folds\n",
    "        \n",
    "        # Cross-Validation Loop\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_spectral)):\n",
    "            \n",
    "            # --- Data Splitting ---\n",
    "            # 1. Spectral\n",
    "            x_train_s, x_val_s = X_spectral[train_idx], X_spectral[val_idx]\n",
    "            \n",
    "            # 2. Target\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # 3. Dynamic Inputs\n",
    "            train_inputs = [x_train_s]\n",
    "            val_inputs   = [x_val_s]\n",
    "            \n",
    "            # Add pH if config requires\n",
    "            if config['ph']:\n",
    "                x_train_p, x_val_p = X_ph[train_idx], X_ph[val_idx]\n",
    "                train_inputs.append(x_train_p)\n",
    "                val_inputs.append(x_val_p)\n",
    "\n",
    "            # Add EC if config requires\n",
    "            if config['ec']:\n",
    "                x_train_e, x_val_e = X_ec[train_idx], X_ec[val_idx]\n",
    "                train_inputs.append(x_train_e)\n",
    "                val_inputs.append(x_val_e)\n",
    "\n",
    "            # --- Build & Train Model ---\n",
    "            # (Assuming build_flexible_model is defined elsewhere in your code)\n",
    "            model = build_flexible_model(\n",
    "                input_shape=(X_spectral.shape[1], 1),\n",
    "                use_ph=config['ph'],\n",
    "                use_ec=config['ec']\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                x=train_inputs,\n",
    "                y=y_train,\n",
    "                validation_data=(val_inputs, y_val),\n",
    "                epochs=30,\n",
    "                batch_size=32,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # --- Evaluation ---\n",
    "            preds = model.predict(val_inputs, verbose=0).flatten()\n",
    "            \n",
    "            # Calculate Metrics\n",
    "            mae = mean_absolute_error(y_val, preds)\n",
    "            r2  = r2_score(y_val, preds)\n",
    "            \n",
    "            fold_maes.append(mae)\n",
    "            fold_r2s.append(r2)\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "        # --- Aggregate Config Results ---\n",
    "        avg_mae = np.mean(fold_maes)\n",
    "        avg_r2  = np.mean(fold_r2s)\n",
    "        min_pred = np.min(all_preds)\n",
    "        max_pred = np.max(all_preds)\n",
    "        \n",
    "        print(f\"   Avg MAE: {avg_mae:.4f} | Avg RÂ²: {avg_r2:.4f}\")\n",
    "        print(f\"   Output Range: [{min_pred:.2f}, {max_pred:.2f}]\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        summary_results.append({\n",
    "            \"Config\": config['name'],\n",
    "            \"MAE\": avg_mae,\n",
    "            \"R2\": avg_r2,\n",
    "            \"Range\": f\"{min_pred:.2f} - {max_pred:.2f}\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary_results)\n",
    "\n",
    "# --- Usage Example ---\n",
    "# Assuming you have your full arrays ready: X_all_spectral, X_all_ph, X_all_ec, y_all_nitrogen\n",
    "\n",
    "# results_df = run_spectral_experiment(\n",
    "#     X_spectral=X_all_spectral, \n",
    "#     X_ph=X_all_ph, \n",
    "#     X_ec=X_all_ec, \n",
    "#     y=y_all_nitrogen, \n",
    "#     feature_name=\"Nitrogen Content\"\n",
    "# )\n",
    "\n",
    "# print(\"\\nFinal Summary Table:\")\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "096f2586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Define your 4 configurations\\n\\nconfigs = [\\n    {\"name\": \"Spectral Only\",      \"ph\": False, \"ec\": False},\\n    {\"name\": \"Spectral + pH\",      \"ph\": True,  \"ec\": False},\\n    {\"name\": \"Spectral + EC\",      \"ph\": False, \"ec\": True},\\n    {\"name\": \"Spectral + pH + EC\", \"ph\": True,  \"ec\": True}\\n]\\n\\n# Dictionary to store results\\nresults = {}\\nprint(\"Starting Experiment Loop...\\n\")\\n\\nfor config in configs:\\n    print(f\"--- Training Model: {config[\\'name\\']} ---\")\\n    \\n    # 1. Build Model\\n    model = build_flexible_model(\\n        input_shape=(df_spectra_snv.shape[1], 1),\\n        use_ph=config[\\'ph\\'],\\n        use_ec=config[\\'ec\\']\\n    )\\n    \\n    # 2. Prepare Dynamic Input Lists\\n    # Always start with spectral\\n    train_inputs = [X_train_spectral]\\n    val_inputs   = [X_val_spectral]\\n    \\n    #---if ph and/or ec is as an input, it will be scaled otherwise untouched \\n    if config[\\'ph\\']:\\n    \\n        train_inputs.append(X_train_ph)\\n        val_inputs.append(X_val_ph)\\n        \\n    if config[\\'ec\\']:\\n        train_inputs.append(X_train_ec)\\n        val_inputs.append(X_val_ec)\\n        \\n    # 3. Train\\n    history = model.fit(\\n        x=train_inputs,\\n        y=y_train,\\n        validation_data=(val_inputs, y_val),\\n        epochs=30,     # Adjust as needed\\n        batch_size=32,\\n        verbose=0      # Silent mode to keep output clean\\n    )\\n    \\n    # 4. Store final validation MAE\\n    final_mae = history.history[\\'val_mae\\'][-1]\\n    results[config[\\'name\\']] = final_mae\\n    print(f\"Final Validation MAE: {final_mae:.4f}\\n\")\\n\\n# --- Summary ---\\nprint(\"--- Experiment Results ---\")\\nfor name, mae in results.items():\\n    print(f\"{name}: {mae:.4f}\")'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Define your 4 configurations\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"Spectral Only\",      \"ph\": False, \"ec\": False},\n",
    "    {\"name\": \"Spectral + pH\",      \"ph\": True,  \"ec\": False},\n",
    "    {\"name\": \"Spectral + EC\",      \"ph\": False, \"ec\": True},\n",
    "    {\"name\": \"Spectral + pH + EC\", \"ph\": True,  \"ec\": True}\n",
    "]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "print(\"Starting Experiment Loop...\\n\")\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"--- Training Model: {config['name']} ---\")\n",
    "    \n",
    "    # 1. Build Model\n",
    "    model = build_flexible_model(\n",
    "        input_shape=(df_spectra_snv.shape[1], 1),\n",
    "        use_ph=config['ph'],\n",
    "        use_ec=config['ec']\n",
    "    )\n",
    "    \n",
    "    # 2. Prepare Dynamic Input Lists\n",
    "    # Always start with spectral\n",
    "    train_inputs = [X_train_spectral]\n",
    "    val_inputs   = [X_val_spectral]\n",
    "    \n",
    "    #---if ph and/or ec is as an input, it will be scaled otherwise untouched \n",
    "    if config['ph']:\n",
    "    \n",
    "        train_inputs.append(X_train_ph)\n",
    "        val_inputs.append(X_val_ph)\n",
    "        \n",
    "    if config['ec']:\n",
    "        train_inputs.append(X_train_ec)\n",
    "        val_inputs.append(X_val_ec)\n",
    "        \n",
    "    # 3. Train\n",
    "    history = model.fit(\n",
    "        x=train_inputs,\n",
    "        y=y_train,\n",
    "        validation_data=(val_inputs, y_val),\n",
    "        epochs=30,     # Adjust as needed\n",
    "        batch_size=32,\n",
    "        verbose=0      # Silent mode to keep output clean\n",
    "    )\n",
    "    \n",
    "    # 4. Store final validation MAE\n",
    "    final_mae = history.history['val_mae'][-1]\n",
    "    results[config['name']] = final_mae\n",
    "    print(f\"Final Validation MAE: {final_mae:.4f}\\n\")\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"--- Experiment Results ---\")\n",
    "for name, mae in results.items():\n",
    "    print(f\"{name}: {mae:.4f}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1187bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers, models\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef build_and_train_cnn(X_train, y_train, X_val, y_val, feature_name):\\n    \"\"\"\\n    Builds and trains a 1D-CNN regressor for a specific soil feature.\\n    \"\"\"\\n    # Reshape input for 1D CNN: (samples, steps, channels)\\n    # Spectral data usually has 1 channel\\n    X_train_reshaped = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\\n    X_val_reshaped = X_val.values.reshape(X_val.shape[0], X_val.shape[1], 1)\\n\\n    model = models.Sequential([\\n        # First Layer: Detects local spectral patterns\\n        layers.Conv1D(filters=32, kernel_size=3, activation=\\'relu\\', input_shape=(X_train.shape[1], 1)),\\n        #layers.MaxPooling1D(pool_size=2),\\n        \\n        # Second Layer: Higher-level feature extraction\\n        layers.Conv1D(filters=64, kernel_size=3, activation=\\'relu\\'),\\n        layers.GlobalAveragePooling1D(), # Reduces dimensionality to prevent overfitting\\n        \\n        # Dense layers for regression\\n        layers.Dense(64, activation=\\'relu\\'),\\n        layers.Dropout(0.2), # Regularization\\n        layers.Dense(32, activation=\\'relu\\'),\\n        layers.Dense(1) # Final regression output\\n    ])\\n\\n    model.compile(optimizer=\\'adam\\', loss=\\'mse\\', metrics=[\\'mae\\'])\\n    \\n    print(f\"\\n--- Training Model for Target: {feature_name} ---\")\\n    history = model.fit(\\n        X_train_reshaped, y_train,\\n        validation_data=(X_val_reshaped, y_val),\\n        epochs=50,\\n        batch_size=16,\\n        verbose=1\\n    )\\n    \\n    return model, history\\n\\n# --- Iterative Execution Logic ---\\n\\n# Assuming \\'df\\' contains: spectral_cols, \\'ph\\', \\'ec\\', and targets [\\'N\\', \\'P\\', \\'K\\', \\'OC\\']\\nspectral_columns = [col for col in df.columns if col.startswith(\\'wave_\\')] # adjust name\\ntarget_features = [\\'N\\', \\'P\\', \\'K\\', \\'Organic_Carbon\\'] \\n\\n# Standardize spectral data\\nscaler = StandardScaler()\\nX = scaler.fit_transform(df[spectral_columns])\\nX = pd.DataFrame(X)\\n\\ntrained_models = {}\\n\\nfor target in target_features:\\n    y = df[target]\\n    \\n    # Split data\\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n    \\n    # Train\\n    model, history = build_and_train_cnn(X_train, y_train, X_val, y_val, target)\\n    \\n    # Store the model for later prediction\\n    trained_models[target] = model'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def build_and_train_cnn(X_train, y_train, X_val, y_val, feature_name):\n",
    "    \"\"\"\n",
    "    Builds and trains a 1D-CNN regressor for a specific soil feature.\n",
    "    \"\"\"\n",
    "    # Reshape input for 1D CNN: (samples, steps, channels)\n",
    "    # Spectral data usually has 1 channel\n",
    "    X_train_reshaped = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_val_reshaped = X_val.values.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "\n",
    "    model = models.Sequential([\n",
    "        # First Layer: Detects local spectral patterns\n",
    "        layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        #layers.MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        # Second Layer: Higher-level feature extraction\n",
    "        layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        layers.GlobalAveragePooling1D(), # Reduces dimensionality to prevent overfitting\n",
    "        \n",
    "        # Dense layers for regression\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2), # Regularization\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1) # Final regression output\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    print(f\"\\n--- Training Model for Target: {feature_name} ---\")\n",
    "    history = model.fit(\n",
    "        X_train_reshaped, y_train,\n",
    "        validation_data=(X_val_reshaped, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=16,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# --- Iterative Execution Logic ---\n",
    "\n",
    "# Assuming 'df' contains: spectral_cols, 'ph', 'ec', and targets ['N', 'P', 'K', 'OC']\n",
    "spectral_columns = [col for col in df.columns if col.startswith('wave_')] # adjust name\n",
    "target_features = ['N', 'P', 'K', 'Organic_Carbon'] \n",
    "\n",
    "# Standardize spectral data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df[spectral_columns])\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for target in target_features:\n",
    "    y = df[target]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train\n",
    "    model, history = build_and_train_cnn(X_train, y_train, X_val, y_val, target)\n",
    "    \n",
    "    # Store the model for later prediction\n",
    "    trained_models[target] = model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21350858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
